{
    "opening": [
        "'speaker_B': 'OK.'",
        "'speaker_C': 'Oh, I don't'",
        "'speaker_A': 'I think I'm zero.'",
        "'speaker_B': 'Wow!'",
        "'speaker_E': 'Ah'",
        "'speaker_F': 'Wh what causes the crash?'",
        "'speaker_B': 'Unprecedented.'",
        "'speaker_C': 'Hello, hello, hello, hello.'",
        "'speaker_A': 'Did you fix something?'",
        "'speaker_C': 'Hello.'",
        "'speaker_E': 'Five, five.'",
        "'speaker_C': 'Hello, hello.'",
        "'speaker_F': 'Oh, maybe it's the turning turning off and turning on of the mike, right?'",
        "'speaker_B': 'Uh, you think that's you?'",
        "'speaker_B': 'Oh.'",
        "'speaker_C': 'Aaa aaa.'",
        "'speaker_F': 'Yeah, OK, mine's working.'",
        "'speaker_C': 'OK.'",
        "'speaker_C': 'That's me.'",
        "'speaker_B': 'OK.'",
        "'speaker_B': 'OK.'",
        "'speaker_B': 'So, um I guess we are'",
        "'speaker_B': 'um gonna do the digits at the end.'",
        "'speaker_B': 'Uh'",
        "'speaker_D': 'Channel channel three, yeah.'",
        "'speaker_E': 'Mmm, channel five?'",
        "'speaker_D': 'OK.'",
        "'speaker_C': 'Channel two.'",
        "'speaker_E': 'Doesn't work?'",
        "'speaker_C': 'Two.'",
        "'speaker_B': 'Yeah, that's the mike number there,'",
        "'speaker_B': 'uh'",
        "'speaker_E': 'No?'",
        "'speaker_A': 'Is it written on her sheet, I believe.'",
        "'speaker_D': 'Mike four.'",
        "'speaker_F': 'Watch this.'",
        "'speaker_B': 'Uh, mike number five,'",
        "'speaker_E': 'Ah, era el cuatro.'",
        "'speaker_F': 'Yep, that's me.'",
        "'speaker_B': 'and channel channel four.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_A': 'But, channel'",
        "'speaker_E': 'Yeah yeah yeah.'",
        "'speaker_B': 'This is you.'",
        "'speaker_E': 'OK.'",
        "'speaker_E': 'I saw that.'",
        "'speaker_E': 'Ah yeah, it's OK.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'And I'm channel uh two I think, or channel'",
        "'speaker_C': 'Ooo.'",
        "'speaker_C': 'I think I'm channel two.'",
        "'speaker_B': 'Oh, I'm channel must be channel one.'",
        "'speaker_E': 'Channel'",
        "'speaker_B': 'Channel one?'",
        "'speaker_B': 'Yes, OK.'",
        "'speaker_B': 'OK.'",
        "'speaker_B': 'So uh'",
        "'speaker_B': 'I also copied uh the results that we all got in the mail I think from uh'",
        "'speaker_B': 'from OGI and we'll go go through them also.'"
    ],
    "neural net test results": [
        "'speaker_B': 'So where are we on'",
        "'speaker_B': 'on uh'",
        "'speaker_B': 'our runs?'",
        "'speaker_D': 'Uh so.'",
        "'speaker_D': 'uh'",
        "'speaker_D': 'We So As I was already said, we we mainly focused on'",
        "'speaker_D': 'uh four kind of features.'",
        "'speaker_B': 'Excuse me.'",
        "'speaker_D': 'The PLP,'",
        "'speaker_D': 'the PLP with JRASTA,'",
        "'speaker_D': 'the MSG, and the MFCC from the baseline Aurora.'",
        "'speaker_E': 'I decided to talk about that.'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_D': 'Uh, and we focused for the the test part on'",
        "'speaker_D': 'the English and the Italian.'",
        "'speaker_D': 'Um.'",
        "'speaker_D': 'We've trained uh several neural networks on'",
        "'speaker_D': 'so on the TI digits English'",
        "'speaker_D': 'and on the Italian data and also on the broad uh'",
        "'speaker_D': 'English'",
        "'speaker_D': 'uh French and uh Spanish databases.'",
        "'speaker_D': 'Mmm,'",
        "'speaker_D': 'so there's our result tables here,'",
        "'speaker_D': 'for the tandem approach, and um,'",
        "'speaker_D': 'actually what we we observed is that'",
        "'speaker_D': 'if the network is trained on the task data'",
        "'speaker_D': 'it works pretty well.'",
        "'speaker_B': 'OK.'",
        "'speaker_C': 'Chicken on the grill.'",
        "'speaker_B': 'Our our uh'",
        "'speaker_B': 'There's a We're pausing for a photo'",
        "'speaker_C': 'Try that corner.'",
        "'speaker_A': 'How about over th from the front of the room?'",
        "'speaker_C': 'Yeah, it's longer.'",
        "'speaker_B': 'We're pausing for a photo opportunity here.'",
        "'speaker_B': 'Uh.'",
        "'speaker_B': 'Uh.'",
        "'speaker_B': 'So.'",
        "'speaker_F': 'Oh wait wait wait wait wait.'",
        "'speaker_C': 'Get out of the Yeah.'",
        "'speaker_F': 'Wait.'",
        "'speaker_F': 'Hold on.'",
        "'speaker_F': 'Hold on.'",
        "'speaker_B': 'OK.'",
        "'speaker_F': 'Let me give you a black screen.'",
        "'speaker_B': 'He's facing this way.'",
        "'speaker_B': 'What?'",
        "'speaker_B': 'OK, this this would be a'",
        "'speaker_F': 'OK.'",
        "'speaker_B': 'good section for our silence detection.'",
        "'speaker_C': 'Mm hmm.'",
        "'speaker_B': 'Um'",
        "'speaker_F': 'Musical chairs everybody!'",
        "'speaker_B': 'Oh.'",
        "'speaker_B': 'OK.'",
        "'speaker_B': 'So um,'",
        "'speaker_B': 'you were saying'",
        "'speaker_B': 'about the training data'",
        "'speaker_D': 'Yeah, so if the network is trained on the task data um tandem works pretty well.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_D': 'And uh actually we have'",
        "'speaker_D': 'uh, results are similar'",
        "'speaker_A': 'Do you mean if it's trained only on'",
        "'speaker_D': 'Only on, yeah.'",
        "'speaker_A': 'On data from just that task, that language?'",
        "'speaker_D': 'Just that task.'",
        "'speaker_D': 'But actually we didn't train network on'",
        "'speaker_D': 'uh both types of data I mean'",
        "'speaker_D': 'uh'",
        "'speaker_D': 'phonetically ba phonetically balanced uh data and task data.'",
        "'speaker_D': 'We only did either task task data or'",
        "'speaker_A': 'Mmm.'",
        "'speaker_D': 'uh broad'",
        "'speaker_D': 'data.'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_D': 'Um'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'So,'",
        "'speaker_B': 'So how I mean clearly it's gonna be good then but the question is how much worse is it if you have broad data?'",
        "'speaker_A': 'So what's th'",
        "'speaker_B': 'I mean, my assump From what I saw from the earlier results, uh I guess last week,'",
        "'speaker_B': 'was that um,'",
        "'speaker_B': 'if you trained on one language and tested on another, say, that the results were were relatively poor.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'But but the question is if you train on one language but you have a broad coverage and then test in another, does that is that improve things'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'i c in comparison?'",
        "'speaker_D': 'If we use the same language?'",
        "'speaker_B': 'No, no, no.'",
        "'speaker_B': 'Different lang So um If you train on TI digits and test on Italian digits, you do poorly, let's'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'say.'",
        "'speaker_B': 'I don't have the numbers in front of me, so I'm just imagining.'",
        "'speaker_D': 'But'",
        "'speaker_D': 'Yeah but I did not uh do'",
        "'speaker_B': 'E'",
        "'speaker_B': 'So, you didn't train on TIMIT and test on on Italian digits, say?'",
        "'speaker_D': 'that.'",
        "'speaker_D': 'We'",
        "'speaker_D': 'No, we did four four kind of of testing, actually.'",
        "'speaker_D': 'The first testing is with task data'",
        "'speaker_D': 'So, with nets trained on task data.'",
        "'speaker_D': 'So for Italian on the Italian'",
        "'speaker_D': 'speech.'",
        "'speaker_D': 'The second test is trained on a single language'",
        "'speaker_D': 'um with broad database, but the same language as the t task data.'",
        "'speaker_B': 'OK.'",
        "'speaker_D': 'But for Italian we choose Spanish which we assume is close to Italian.'",
        "'speaker_D': 'The third test is by using, um'",
        "'speaker_D': 'the three language database'",
        "'speaker_D': 'and the fourth is'",
        "'speaker_B': 'W which in It has three languages.'",
        "'speaker_B': 'That's including the w the the'",
        "'speaker_D': 'This includes'",
        "'speaker_B': 'the one that it's'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'But not'",
        "'speaker_A': 'In'",
        "'speaker_D': 'digits.'",
        "'speaker_D': 'I mean it's'",
        "'speaker_B': 'Right.'",
        "'speaker_A': 'The three languages is not digits, it's the broad data.'",
        "'speaker_D': 'Yeah'",
        "'speaker_A': 'OK.'",
        "'speaker_D': 'And the fourth test is uh'",
        "'speaker_D': 'excluding from these three languages the language that is'",
        "'speaker_D': 'the task language.'",
        "'speaker_B': 'Oh, OK, yeah, so, that is what I wanted to know.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'I just wasn't saying it very well, I guess.'",
        "'speaker_D': 'Uh, yeah.'",
        "'speaker_D': 'So'",
        "'speaker_D': 'um for uh TI digits for ins example'",
        "'speaker_D': 'uh'",
        "'speaker_D': 'when we go from'",
        "'speaker_D': 'TI digits training to TIMIT training'",
        "'speaker_D': 'uh we lose'",
        "'speaker_D': 'uh around ten percent, uh.'",
        "'speaker_D': 'The error rate increase u of of of ten percent, relative.'",
        "'speaker_B': 'Relative.'",
        "'speaker_B': 'Right.'",
        "'speaker_D': 'So this is not so bad.'",
        "'speaker_D': 'And then when we jump to the multilingual data it's uh it become worse and, well'",
        "'speaker_B': 'Ab about how much?'",
        "'speaker_D': 'Around uh, let's say,'",
        "'speaker_D': 'twenty perc twenty percent further.'",
        "'speaker_D': 'So.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Twenty percent further?'",
        "'speaker_D': 'Twenty to to thirty percent further.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_A': 'And so, remind me, the multilingual stuff is just the broad data.'",
        "'speaker_A': 'Right?'",
        "'speaker_D': 'Yeah.'",
        "'speaker_A': 'It's not the digits.'",
        "'speaker_A': 'So it's the combination of'",
        "'speaker_A': 'two things there.'",
        "'speaker_A': 'It's removing the task specific training and'",
        "'speaker_A': 'it's adding other languages.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_A': 'OK.'",
        "'speaker_D': 'But the first step is al already removing the task s specific from from So.'",
        "'speaker_A': 'Already, right right right.'",
        "'speaker_D': 'And we lose'",
        "'speaker_A': 'So they were sort of building here?'",
        "'speaker_D': 'Yeah.'",
        "'speaker_A': 'OK?'",
        "'speaker_D': 'Uh'",
        "'speaker_D': 'So, basically when it's trained on the the multilingual broad data'",
        "'speaker_D': 'um or number so, the the'",
        "'speaker_D': 'ratio'",
        "'speaker_D': 'of our error rates'",
        "'speaker_D': 'uh with the baseline error rate is around'",
        "'speaker_D': 'uh one point one.'",
        "'speaker_D': 'So.'",
        "'speaker_B': 'Yes.'",
        "'speaker_B': 'And it's something like one point three'",
        "'speaker_B': 'of of the'",
        "'speaker_B': 'uh I i if you compare everything to the first case at the baseline,'",
        "'speaker_B': 'you get something like one point one for the for the using the same language but a different task, and something like one point three'",
        "'speaker_B': 'for three three languages broad stuff.'",
        "'speaker_D': 'No no no.'",
        "'speaker_D': 'Uh'",
        "'speaker_D': 'same language we are at uh for at English atO point eight.'",
        "'speaker_D': 'So it improves,'",
        "'speaker_D': 'compared to the baseline.'",
        "'speaker_D': 'But'",
        "'speaker_D': 'So.'",
        "'speaker_D': 'Le let me.'",
        "'speaker_B': 'I I I'm sorry.'",
        "'speaker_D': 'Tas task data we are u'",
        "'speaker_B': 'I I I meant something different by baseline So let me let me'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Um,'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'so,'",
        "'speaker_B': 'um'",
        "'speaker_B': 'OK, fine.'",
        "'speaker_B': 'Let's let's use the conventional meaning of baseline.'",
        "'speaker_D': 'Hmm.'",
        "'speaker_B': 'I I By baseline here I meant'",
        "'speaker_B': 'uh using the task specific data.'",
        "'speaker_D': 'Oh yeah, the f'",
        "'speaker_D': 'Yeah, OK.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'But uh uh, because that's what you were just doing with this ten percent.'",
        "'speaker_B': 'So I was just I just trying to understand that.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'Sure.'",
        "'speaker_B': 'So if we call'",
        "'speaker_B': 'a factor of w just one, just normalized to one, the word error rate that you have for using TI digits as as'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'training and TI digits as test,'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'uh different words, I'm sure, but but uh, uh the same task and so on.'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'If we call that one\", then what you're saying is that the word error rate for the same language but using uh different training data than you're testing on, say TIMIT and so forth, it's'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'one point one.'",
        "'speaker_D': 'Yeah, it's around one point one.'",
        "'speaker_B': 'Right.'",
        "'speaker_B': 'And if it's you do go to three languages including the English, it's something like one point three.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'Ye'",
        "'speaker_B': 'That's what you were just saying, I think.'",
        "'speaker_D': 'Uh, more actually.'",
        "'speaker_A': 'One point four?'",
        "'speaker_D': 'If I Yeah.'",
        "'speaker_A': 'So, it's an additional thirty percent.'",
        "'speaker_D': 'What would you say?'",
        "'speaker_D': 'Around one point four yeah.'",
        "'speaker_B': 'OK.'",
        "'speaker_B': 'And if you exclude English, from this combination, what's that?'",
        "'speaker_D': 'If we exclude English, um'",
        "'speaker_D': 'there is not much difference with the'",
        "'speaker_D': 'data with English.'",
        "'speaker_D': 'So.'",
        "'speaker_B': 'Aha!'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'That's interesting.'",
        "'speaker_B': 'That's interesting.'",
        "'speaker_B': 'Do you see?'",
        "'speaker_B': 'Because Uh, so No, that that's important.'",
        "'speaker_D': 'Uh.'",
        "'speaker_B': 'So what what it's saying here is just that\" yes, there is a reduction in performance, when you don't um'",
        "'speaker_B': 'have the s when you don't have um'",
        "'speaker_A': 'Task data.'",
        "'speaker_B': 'Wait a minute, th th the'",
        "'speaker_D': 'Hmm.'",
        "'speaker_B': 'No, actually it's interesting.'",
        "'speaker_B': 'So it's So when you go to a different task, there's actually not so different.'",
        "'speaker_B': 'It's when you went to these So what's the difference between two and three?'",
        "'speaker_B': 'Between the one point one case and the one point four case?'",
        "'speaker_B': 'I'm confused.'",
        "'speaker_A': 'It's multilingual.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'The only difference it's is that it's multilingual'",
        "'speaker_D': 'Um Yeah.'",
        "'speaker_B': 'Cuz in both in both both of those cases, you don't have the same task.'",
        "'speaker_D': 'Yeah sure.'",
        "'speaker_B': 'So is is the training data for the for this one point four case'",
        "'speaker_B': 'does it include the training data for the one point one case?'",
        "'speaker_D': 'Uh yeah.'",
        "'speaker_F': 'Yeah, a fraction of it.'",
        "'speaker_D': 'A part of it, yeah.'",
        "'speaker_B': 'How m how much bigger is it?'",
        "'speaker_D': 'Um'",
        "'speaker_F': 'Yeah, um.'",
        "'speaker_D': 'It's two times, actually?'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'Um.'",
        "'speaker_D': 'The'",
        "'speaker_D': 'English data No, the multilingual databases are two times the broad English data.'",
        "'speaker_D': 'We just wanted to keep this, w well, not too huge.'",
        "'speaker_D': 'So.'",
        "'speaker_B': 'So it's two times, but it includes the but it includes the broad English data.'",
        "'speaker_D': 'I think so.'",
        "'speaker_D': 'Do you'",
        "'speaker_D': 'Uh,'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'And the broad English data is what you got this one point one with.'",
        "'speaker_B': 'So that's TIMIT basically right?'",
        "'speaker_D': 'Yeah.'",
        "'speaker_F': 'Mm hmm.'",
        "'speaker_B': 'So it's band limited TIMIT.'",
        "'speaker_F': 'Mm hmm.'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'This is all'",
        "'speaker_D': 'Yeah.'",
        "'speaker_F': 'Downs Right.'",
        "'speaker_B': 'eight kilohertz sampling.'",
        "'speaker_B': 'So you have band limited TIMIT, gave you uh almost as good as a result as using TI digits on a TI digits test.'",
        "'speaker_B': 'OK?'",
        "'speaker_D': 'Hmm?'",
        "'speaker_B': 'Um and um'",
        "'speaker_B': 'But,'",
        "'speaker_B': 'when you add in more training data but keep the neural net the same size,'",
        "'speaker_B': 'it um performs worse on the TI digits.'",
        "'speaker_B': 'OK, now all of this is This is noisy TI digits, I assume?'",
        "'speaker_B': 'Both training and test?'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'OK.'",
        "'speaker_B': 'Um'",
        "'speaker_B': 'OK.'",
        "'speaker_B': 'Well.'",
        "'speaker_B': 'We we we may just need to uh'",
        "'speaker_B': 'So I mean it's interesting that h going to a different different task didn't seem to hurt us that much, and going to a different language'",
        "'speaker_B': 'um'",
        "'speaker_B': 'It doesn't seem to matter The difference between three and four is not particularly great, so that means that'",
        "'speaker_B': 'whether you have the language in or not is not such a big deal.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'It sounds like um'",
        "'speaker_B': 'uh'",
        "'speaker_B': 'we may need to have more'",
        "'speaker_B': 'of uh things that are similar to a target language or I mean.'",
        "'speaker_B': 'You have the same number of parameters in the neural net, you haven't increased the size of the neural net, and maybe there's just just not enough'",
        "'speaker_B': 'complexity to it to represent the variab increased variability in the in the training set.'",
        "'speaker_B': 'That that could be.'",
        "'speaker_B': 'Um So, what about So these are results with'",
        "'speaker_B': 'uh th that you're describing now, that they are pretty similar for the different features or or uh'",
        "'speaker_D': 'Uh, let me check.'",
        "'speaker_D': 'Uh.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_D': 'So.'",
        "'speaker_D': 'This was for the PLP,'",
        "'speaker_B': 'Yeah.'",
        "'speaker_D': 'Um.'",
        "'speaker_D': 'The Yeah.'",
        "'speaker_D': 'For the PLP with JRASTA the the we This is quite the same'",
        "'speaker_D': 'tendency,'",
        "'speaker_D': 'with a slight increase of the error rate, uh if we go to to TIMIT.'",
        "'speaker_D': 'And then it's it gets worse with the multilingual.'",
        "'speaker_D': 'Um.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'There there is a difference actually with b between PLP and JRASTA is that JRASTA'",
        "'speaker_D': 'seems to perform better with the highly mismatched condition'",
        "'speaker_D': 'but slightly slightly worse for the well matched condition.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'I have a suggestion, actually, even though it'll delay us slightly, would would you mind running into the other room and making'",
        "'speaker_B': 'copies of this?'",
        "'speaker_B': 'Cuz we're all sort of If we c if we could look at it, while we're talking, I think it'd be uh'",
        "'speaker_D': 'Yeah, yeah.'",
        "'speaker_D': 'OK.'",
        "'speaker_B': 'Uh, I'll I'll sing a song or dance or something while you'",
        "'speaker_F': 'Alright.'",
        "'speaker_A': 'So um Go ahead.'",
        "'speaker_B': 'do it, too.'",
        "'speaker_A': 'Ah, while you're gone I'll ask s some of my questions.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_A': 'Um.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'Uh,'",
        "'speaker_B': 'this way and just slightly to the left, yeah.'",
        "'speaker_A': 'The um'",
        "'speaker_A': 'What was Was this number forty or It was roughly the same as this one, he said?'",
        "'speaker_B': 'Um.'",
        "'speaker_A': 'When you had the two language versus the three language?'",
        "'speaker_B': 'That's what he was saying.'",
        "'speaker_A': 'That's where he removed English, right?'",
        "'speaker_F': 'Yeah.'",
        "'speaker_B': 'Right.'",
        "'speaker_F': 'It sometimes, actually, depends on what features you're using.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'But but i it sounds like'",
        "'speaker_F': 'Um, but'",
        "'speaker_B': 'I mean.'",
        "'speaker_B': 'That's interesting because it it seems like what it's saying is not so much that you got hurt'",
        "'speaker_B': 'uh because you'",
        "'speaker_B': 'uh didn't have so much representation of English,'",
        "'speaker_B': 'because in the other case you don't get hurt any more, at least when it seemed like uh it it might simply be a case that you have something that is just much more diverse, but you have the same number of parameters representing it.'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_A': 'I wonder were um all three of these nets using the same output?'",
        "'speaker_A': 'This multi language'",
        "'speaker_F': 'He'",
        "'speaker_F': 'Mm hmm.'",
        "'speaker_A': 'uh labelling?'",
        "'speaker_F': 'He was using uh sixty four phonemes from SAMPA.'",
        "'speaker_A': 'OK, OK.'",
        "'speaker_F': 'Yeah.'",
        "'speaker_A': 'So this would From this you would say, well, it doesn't really matter if we put Finnish'",
        "'speaker_A': 'into the training of the neural net, if there's gonna be,'",
        "'speaker_A': 'you know, Finnish in the test data.'",
        "'speaker_A': 'Right?'",
        "'speaker_B': 'Well, it's it sounds I mean, we have to be careful, cuz we haven't gotten a good result yet.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_B': 'And comparing different bad results can be tricky.'",
        "'speaker_A': 'Hmm.'",
        "'speaker_B': 'But I I I I think it does suggest that it's not so much uh'",
        "'speaker_B': 'uh cross'",
        "'speaker_B': 'language as cross type of speech.'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_B': 'It's it's um'",
        "'speaker_B': 'But we did Oh yeah, the other thing I was asking him, though, is that I think that in the case'",
        "'speaker_B': 'Yeah, you you do have to be careful because of com compounded results.'",
        "'speaker_B': 'I think we got some earlier results'",
        "'speaker_B': 'in which you trained on one language and tested on another and you didn't have three, but you just had one language.'",
        "'speaker_B': 'So you trained on'",
        "'speaker_B': 'one type of digits and tested on another.'",
        "'speaker_B': 'Didn Wasn't there something of that?'",
        "'speaker_B': 'Where you, say, trained on Spanish and tested on on TI digits, or the other way around?'",
        "'speaker_B': 'Something like that?'",
        "'speaker_E': 'No.'",
        "'speaker_B': 'I thought there was something like that, that he showed me last week.'",
        "'speaker_B': 'We'll have to wait till we get'",
        "'speaker_A': 'Yeah, that would be interesting.'",
        "'speaker_B': 'Um,'",
        "'speaker_B': 'This may have been what I was asking before, Stephane, but but, um, wasn't there something that you did, where you trained on one language and tested on another?'",
        "'speaker_B': 'I mean no no mixture but just'",
        "'speaker_F': 'I'll get it for you.'",
        "'speaker_D': 'Uh, no,'",
        "'speaker_D': 'no.'",
        "'speaker_B': 'We've never just trained on one lang'",
        "'speaker_D': 'Training on a single language, you mean, and testing on the other one?'",
        "'speaker_B': 'Yeah.'",
        "'speaker_E': 'Not yet.'",
        "'speaker_D': 'Uh, no.'",
        "'speaker_D': 'So the only task that's similar to this is the training on two languages, and'",
        "'speaker_B': 'But we've done a bunch of things where we just trained on one language.'",
        "'speaker_D': 'that'",
        "'speaker_B': 'Right?'",
        "'speaker_B': 'I mean, you haven't you haven't done all your tests on multiple languages.'",
        "'speaker_D': 'Uh,'",
        "'speaker_D': 'No.'",
        "'speaker_D': 'Either thi this is test with'",
        "'speaker_D': 'uh the same language but from the broad data, or it's test with'",
        "'speaker_D': 'uh different languages also from the broad data, excluding the So, it's it's three or three and four.'",
        "'speaker_E': 'The early experiment that'",
        "'speaker_A': 'Did you do different languages from digits?'",
        "'speaker_D': 'Uh.'",
        "'speaker_D': 'No.'",
        "'speaker_D': 'You mean'",
        "'speaker_D': 'training digits'",
        "'speaker_D': 'on one language and using the net to recognize on the other?'",
        "'speaker_A': 'Digits on another language?'",
        "'speaker_D': 'No.'",
        "'speaker_B': 'See, I thought you showed me something like that last week.'",
        "'speaker_B': 'You had a you had a little'",
        "'speaker_D': 'Uh,'",
        "'speaker_D': 'No, I don't think so.'",
        "'speaker_B': 'Um'",
        "'speaker_B': 'What'",
        "'speaker_C': 'These numbers are uh ratio to baseline?'",
        "'speaker_B': 'So, I mean wha what's the This this chart this table that we're looking at'",
        "'speaker_D': 'So.'",
        "'speaker_B': 'is um,'",
        "'speaker_B': 'show is all testing for TI digits, or?'",
        "'speaker_F': 'Bigger is worse.'",
        "'speaker_D': 'So you have uh basically two uh parts.'",
        "'speaker_F': 'This is error rate, I think.'",
        "'speaker_C': 'Ratio.'",
        "'speaker_F': 'No.'",
        "'speaker_F': 'No.'",
        "'speaker_D': 'The upper part is for TI digits and it's divided in three rows'",
        "'speaker_F': 'Yeah, yeah, yeah.'",
        "'speaker_F': 'Mm hmm.'",
        "'speaker_D': 'of four four rows each.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_D': 'And the first four rows is well matched, then the s the second group of four rows is mismatched, and'",
        "'speaker_D': 'finally highly mismatched.'",
        "'speaker_D': 'And then the lower part is for Italian and it's the same'",
        "'speaker_D': 'the same thing.'",
        "'speaker_A': 'So, so the upper part is training TI digits?'",
        "'speaker_D': 'So.'",
        "'speaker_D': 'It's it's the HTK results, I mean.'",
        "'speaker_D': 'So it's'",
        "'speaker_A': 'Ah.'",
        "'speaker_D': 'HTK training testings'",
        "'speaker_D': 'with different kind of features and what appears in the'",
        "'speaker_D': 'uh left column is the networks that are used for doing this.'",
        "'speaker_B': 'Hmm.'",
        "'speaker_D': 'So.'",
        "'speaker_D': 'Uh'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Well,'",
        "'speaker_B': 'What was is that i What was it that you had'",
        "'speaker_B': 'done last week when you showed Do you remember?'",
        "'speaker_B': 'Wh when you showed me the your table last week?'",
        "'speaker_D': 'It It was part of these results.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_A': 'So where is the baseline for the TI digits located in here?'",
        "'speaker_D': 'You mean the HTK Aurora baseline?'",
        "'speaker_A': 'Yeah.'",
        "'speaker_D': 'It's uh the one hundred number.'",
        "'speaker_D': 'It's, well, all these numbers are the ratio'",
        "'speaker_A': 'Ah!'",
        "'speaker_D': 'with respect to the baseline.'",
        "'speaker_A': 'Ah, OK, OK.'",
        "'speaker_B': 'So this is word word error rate, so a high number is bad.'",
        "'speaker_D': 'Yeah, this is'",
        "'speaker_E': 'Yeah.'",
        "'speaker_D': 'a word error rate ratio.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_A': 'OK, I see.'",
        "'speaker_D': 'So,'",
        "'speaker_D': 'seventy point two means that'",
        "'speaker_D': 'we reduced the error rate uh by thirty thirty percent.'",
        "'speaker_D': 'So.'",
        "'speaker_A': 'OK, OK, gotcha.'",
        "'speaker_D': 'Hmm.'",
        "'speaker_B': 'OK,'",
        "'speaker_B': 'so if we take uh'",
        "'speaker_B': 'um let's see'",
        "'speaker_B': 'PLP uh with on line normalization and delta del so that's this thing you have circled here'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'in the second column,'",
        "'speaker_B': 'um'",
        "'speaker_B': 'and multi English\" refers to what?'",
        "'speaker_D': 'To TIMIT.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_D': 'Then you have'",
        "'speaker_D': 'uh MF,'",
        "'speaker_D': 'MS and ME which are for French, Spanish and English.'",
        "'speaker_D': 'And,'",
        "'speaker_D': 'yeah.'",
        "'speaker_D': 'Actually I I uh forgot to say that the multilingual net are trained on uh features'",
        "'speaker_D': 'without the s derivatives'",
        "'speaker_D': 'uh but with'",
        "'speaker_D': 'increased frame numbers.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_D': 'And we can we can see on the first line of the table that it it'",
        "'speaker_D': 'it's slightly slightly worse when we don't use delta but it's not'",
        "'speaker_B': 'Right.'",
        "'speaker_B': 'So w w So, I'm sorry.'",
        "'speaker_D': 'not that much.'",
        "'speaker_B': 'I missed that.'",
        "'speaker_B': 'What's MF, MS and ME?'",
        "'speaker_A': 'Multi French, Multi Spanish'",
        "'speaker_D': 'So.'",
        "'speaker_D': 'Multi French, Multi Spanish, and Multi English.'",
        "'speaker_B': 'Uh OK.'",
        "'speaker_B': 'So, it's'",
        "'speaker_B': 'uh'",
        "'speaker_B': 'broader vocabulary.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Then'",
        "'speaker_B': 'And OK so I think what I'm what I saw in your smaller chart that I was thinking of was was'",
        "'speaker_B': 'there were some numbers I saw, I think, that included these multiple languages and it and I was seeing that it got worse.'",
        "'speaker_B': 'I I think that was all it was.'",
        "'speaker_B': 'You had some very limited results that at that point'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'which showed having in these these other languages.'",
        "'speaker_B': 'In fact it might have been just this last category, having two languages broad that were where where English was removed.'",
        "'speaker_B': 'So that was cross language and the and the result was quite poor.'",
        "'speaker_B': 'What I we hadn't seen yet was that if you added in the English, it's still poor.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Uh'",
        "'speaker_D': 'Still poor.'",
        "'speaker_B': 'Um now, what's the noise condition um of the training data Well, I think this is what you were explaining.'",
        "'speaker_B': 'The noise condition is the same It's the same uh Aurora noises'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'uh, in all these cases for the training.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'So there's not a statistical sta a strong st statistically different'",
        "'speaker_B': 'noise characteristic between'",
        "'speaker_D': 'No these are the s s s same noises, yeah.'",
        "'speaker_B': 'uh the training and test and yet we're seeing some kind of effect'",
        "'speaker_D': 'At least at least for the first'",
        "'speaker_D': 'for the well matched,'",
        "'speaker_F': 'Well matched condition.'",
        "'speaker_D': 'yeah.'",
        "'speaker_B': 'Right.'",
        "'speaker_B': 'So there's some kind of a a an effect from having these uh this broader coverage'",
        "'speaker_B': 'um'",
        "'speaker_B': 'Now I guess what we should try doing with this is try testing these on u this same sort of thing on you probably must have this lined up to do.'",
        "'speaker_B': 'To try the same t'",
        "'speaker_B': 'with the exact same training, do testing on the other languages.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'On on um'",
        "'speaker_B': 'So.'",
        "'speaker_B': 'Um, oh I well, wait a minute.'",
        "'speaker_B': 'You have this here, for the Italian.'",
        "'speaker_B': 'That's right.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'OK, so, So.'",
        "'speaker_D': 'Yeah, so for the Italian the results are uh'",
        "'speaker_D': 'stranger'",
        "'speaker_D': 'um'",
        "'speaker_D': 'Mmm.'",
        "'speaker_D': 'So what appears is that perhaps Spanish is'",
        "'speaker_D': 'not very close to Italian'",
        "'speaker_D': 'because uh, well,'",
        "'speaker_D': 'when using the the network trained only on Spanish it's'",
        "'speaker_D': 'the error rate is'",
        "'speaker_D': 'almost uh twice'",
        "'speaker_D': 'the baseline error rate.'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'Well, I mean, let's see.'",
        "'speaker_D': 'Uh.'",
        "'speaker_B': 'Is there any difference in'",
        "'speaker_B': 'So it's in the uh'",
        "'speaker_B': 'So you're saying that when you train on English and uh'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'and and test on'",
        "'speaker_B': 'No, you don't have training on English testing'",
        "'speaker_D': 'There there is another difference, is that the noise the noises are different.'",
        "'speaker_D': 'Well,'",
        "'speaker_B': 'In in what?'",
        "'speaker_D': 'For for the Italian part I mean the'",
        "'speaker_D': 'uh the um'",
        "'speaker_D': 'networks are trained with noise from'",
        "'speaker_D': 'Aurora TI digits, mmm.'",
        "'speaker_E': 'Aurora two.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'And the noise is different in th'",
        "'speaker_D': 'And perhaps the noise are'",
        "'speaker_D': 'quite different from the noises in the speech that Italian.'",
        "'speaker_B': 'Do we have any um'",
        "'speaker_D': 'And'",
        "'speaker_B': 'test sets'",
        "'speaker_B': 'uh in any other language that um'",
        "'speaker_B': 'have the same noise as in the Aurora?'",
        "'speaker_E': 'Mmm, no.'",
        "'speaker_D': 'No.'",
        "'speaker_A': 'Can I ask something real quick?'",
        "'speaker_A': 'In in the upper part'",
        "'speaker_A': 'in the English stuff, it looks like the very best number is sixty point nine?'",
        "'speaker_A': 'and that's in the uh the third section in the upper part under PLP JRASTA, sort of the middle column?'",
        "'speaker_D': 'Yeah.'",
        "'speaker_A': 'I is that a noisy condition?'",
        "'speaker_D': 'Yeah.'",
        "'speaker_A': 'So that's matched training?'",
        "'speaker_A': 'Is that what that is?'",
        "'speaker_D': 'It's no, the third part, so it's uh highly mismatched.'",
        "'speaker_D': 'So.'",
        "'speaker_D': 'Training and'",
        "'speaker_A': 'So why do you get your best number'",
        "'speaker_D': 'test noise are different.'",
        "'speaker_A': 'in'",
        "'speaker_A': 'Wouldn't you get your best number in the clean case?'",
        "'speaker_C': 'Well, it's relative to the'",
        "'speaker_C': 'um baseline mismatching'",
        "'speaker_D': 'Yeah.'",
        "'speaker_A': 'Ah, OK so these are not OK, alright, I see.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_A': 'OK.'",
        "'speaker_A': 'And then so, in the in the um'",
        "'speaker_A': 'in the non mismatched clean case, your best one was under MFCC?'",
        "'speaker_A': 'That sixty one point four?'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'But it's not a clean case.'",
        "'speaker_D': 'It's a noisy case but'",
        "'speaker_D': 'uh training and test noises are the same.'",
        "'speaker_A': 'Oh!'",
        "'speaker_A': 'So this upper third?'",
        "'speaker_D': 'So'",
        "'speaker_D': 'Yeah.'",
        "'speaker_A': 'Uh that's still noisy?'",
        "'speaker_D': 'Yeah.'",
        "'speaker_A': 'Ah, OK.'",
        "'speaker_D': 'So it's always noisy basically,'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_D': 'and,'",
        "'speaker_D': 'well,'",
        "'speaker_D': 'the'",
        "'speaker_A': 'I see.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'OK?'",
        "'speaker_B': 'Um'",
        "'speaker_B': 'So uh, I think this will take some looking at, thinking about.'",
        "'speaker_B': 'But, what is uh what is currently running, that's uh, i that just filling in the holes here or or?'",
        "'speaker_D': 'Uh, no we don't plan to fill the holes but actually there is something important,'",
        "'speaker_B': 'pretty much?'",
        "'speaker_B': 'OK.'",
        "'speaker_D': 'is that'",
        "'speaker_D': 'um'",
        "'speaker_D': 'we made a lot of assumption concerning the on line normalization'",
        "'speaker_D': 'and we just noticed'",
        "'speaker_D': 'uh recently that'",
        "'speaker_D': 'uh the'",
        "'speaker_D': 'approach that we were using was not'",
        "'speaker_D': 'uh'",
        "'speaker_D': 'leading to very good results when we used the straight features to HTK.'",
        "'speaker_D': 'Um'",
        "'speaker_D': 'Mmm.'",
        "'speaker_D': 'So basically d if you look at the at the left of the table,'",
        "'speaker_D': 'the first uh row, with eighty six, one hundred, and forty three and seventy five,'",
        "'speaker_D': 'these are the results we obtained for Italian'",
        "'speaker_D': 'uh with'",
        "'speaker_D': 'straight'",
        "'speaker_D': 'mmm, PLP features'",
        "'speaker_D': 'using on line normalization.'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_D': 'And the, mmm'",
        "'speaker_D': 'what's in the table, just at the left of the PLP twelve on line normalization column, so, the numbers seventy nine, fifty four and'",
        "'speaker_D': 'uh forty two'",
        "'speaker_D': 'are the results obtained by uh Pratibha with uh his on line normalization uh her on line normalization approach.'",
        "'speaker_A': 'Where is that?'",
        "'speaker_A': 'seventy nine, fifty'",
        "'speaker_B': 'Uh, it's just sort of sitting right on the uh the column line.'",
        "'speaker_E': 'Fifty one?'",
        "'speaker_D': 'So.'",
        "'speaker_E': 'This'",
        "'speaker_B': 'Uh.'",
        "'speaker_A': 'Oh I see, OK.'",
        "'speaker_D': 'Just uh'",
        "'speaker_B': 'Yeah.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'So these are the results of OGI with on line normalization and straight features to HTK.'",
        "'speaker_D': 'And the previous result, eighty six and so on,'",
        "'speaker_B': 'Yes.'",
        "'speaker_D': 'are with our'",
        "'speaker_D': 'features straight to HTK.'",
        "'speaker_B': 'Yes.'",
        "'speaker_D': 'So what we see that is there is that um'",
        "'speaker_D': 'uh the way we were doing this was not correct, but still the networks are very good.'",
        "'speaker_D': 'When we use the networks'",
        "'speaker_D': 'our number are better that'",
        "'speaker_E': 'We improve.'",
        "'speaker_B': 'So, do you know what was wrong with the on line normalization, or?'",
        "'speaker_D': 'uh Pratibha results.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'There were diff there were different things and basically,'",
        "'speaker_D': 'the first thing is the mmm,'",
        "'speaker_D': 'alpha uh'",
        "'speaker_D': 'value.'",
        "'speaker_D': 'So, the recursion'",
        "'speaker_D': 'uh'",
        "'speaker_D': 'part.'",
        "'speaker_D': 'um,'",
        "'speaker_D': 'I used point five percent, which was the default value in the'",
        "'speaker_D': 'in the programs here.'",
        "'speaker_D': 'And Pratibha used five percent.'",
        "'speaker_D': 'So it adapts more quickly'",
        "'speaker_B': 'Yes.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_D': 'Um, but, yeah.'",
        "'speaker_D': 'I assume that this was not important because'",
        "'speaker_D': 'uh previous results from from Dan and show that basically'",
        "'speaker_D': 'the'",
        "'speaker_D': 'both both values g give the same'",
        "'speaker_D': 'same'",
        "'speaker_D': 'uh'",
        "'speaker_D': 'results.'",
        "'speaker_D': 'It was true on'",
        "'speaker_D': 'uh'",
        "'speaker_D': 'but it's not true on Italian.'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_D': 'Uh, second thing is the initialization of the'",
        "'speaker_D': 'stuff.'",
        "'speaker_D': 'Actually,'",
        "'speaker_D': 'uh what we were doing is to start the recursion from the beginning of the'",
        "'speaker_D': 'utterance.'",
        "'speaker_D': 'And using initial values that are the global mean and variances measured across the whole database.'",
        "'speaker_B': 'Right.'",
        "'speaker_B': 'Right.'",
        "'speaker_D': 'And Pratibha did something different is that he uh she initialed the um'",
        "'speaker_D': 'values of the mean and variance by computing this on the'",
        "'speaker_D': 'twenty five first frames of each utterance.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_D': 'There were other minor differences, the fact that she used fifteen dissities instead s instead of thirteen,'",
        "'speaker_D': 'and that she usedC zero instead of log energy.'",
        "'speaker_D': 'Uh, but the main differences concerns the recursion.'",
        "'speaker_D': 'So.'",
        "'speaker_D': 'Uh, I changed the code'",
        "'speaker_D': 'uh and now we have a baseline that's similar to the OGI baseline.'",
        "'speaker_B': 'OK.'",
        "'speaker_D': 'We It it's slightly uh different because I don't exactly initialize the same way she does.'",
        "'speaker_D': 'Actually I start, mmm, I don't wait to a fifteen twenty five twenty five frames before computing a mean and the variance'",
        "'speaker_C': 'Mm hmm.'",
        "'speaker_D': 'to e to to start the recursion.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_D': 'I I use the on line scheme'",
        "'speaker_D': 'and only start the re recursion after the twenty five twenty fifth frame.'",
        "'speaker_D': 'But, well it's similar.'",
        "'speaker_D': 'So'",
        "'speaker_D': 'uh I retrained the networks with'",
        "'speaker_D': 'these well, the the the networks are retaining with these new'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_D': 'features.'",
        "'speaker_D': 'And, yeah.'",
        "'speaker_B': 'OK.'",
        "'speaker_D': 'So basically what I expect is that these numbers will a little bit go down but'",
        "'speaker_D': 'perhaps not not so much'",
        "'speaker_B': 'Right.'",
        "'speaker_D': 'because I think the neural networks learn perhaps to'",
        "'speaker_D': 'even if the features are not'",
        "'speaker_B': 'Right.'",
        "'speaker_D': 'normalized.'",
        "'speaker_D': 'It it will learn how to normalize and.'",
        "'speaker_B': 'OK, but I think that given the pressure of time we probably want to draw because of that especially, we wanna draw some conclusions from this, do some reductions'",
        "'speaker_B': 'in what we're looking at,'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'and make some strong decisions for what we're gonna do testing on before next week.'",
        "'speaker_D': 'Yeah'",
        "'speaker_B': 'So do you are you w did you have something going on, on the side, with uh multi band or on on this, or?'",
        "'speaker_D': 'No, I we plan to start this'",
        "'speaker_D': 'uh so, act actually we have discussed uh'",
        "'speaker_D': 'um, these'",
        "'speaker_D': 'what we could do'",
        "'speaker_D': 'more as a as a research and'",
        "'speaker_D': 'and'",
        "'speaker_D': 'we were thinking perhaps that'",
        "'speaker_D': 'uh the way we use the tandem is not'",
        "'speaker_D': 'Uh, well, there is basically perhaps a flaw in the in the the stuff because'",
        "'speaker_D': 'we'",
        "'speaker_D': 'trained the networks If we trained the networks on the on'",
        "'speaker_D': 'a language and a t or a specific task,'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_D': 'um, what we ask is to the network is to put the bound the decision boundaries somewhere in the space.'",
        "'speaker_B': 'Mmm.'",
        "'speaker_D': 'And'",
        "'speaker_D': 'and ask the network to put one,'",
        "'speaker_D': 'at one side of the for for a particular phoneme at one side of the boundary decision boundary and one for another phoneme at the other side.'",
        "'speaker_D': 'And so there is kind of reduction of the information there that's not correct because if we change task and if the phonemes are not in the same context in the new task,'",
        "'speaker_D': 'obviously the decision boundaries are not'",
        "'speaker_D': 'should not be at the same'",
        "'speaker_D': 'place.'",
        "'speaker_D': 'But the way the feature gives The the way the network gives the features is that it reduce completely the it removes completely the information'",
        "'speaker_B': 'I di'",
        "'speaker_D': 'a lot of information from the the features'",
        "'speaker_D': 'by uh'",
        "'speaker_D': 'uh'",
        "'speaker_D': 'placing the decision boundaries at optimal places for one kind of data but this is not the case for another kind of data.'",
        "'speaker_B': 'It's a trade off, right?'",
        "'speaker_D': 'So'",
        "'speaker_B': 'Any anyway go ahead.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'So uh what we were thinking about is perhaps'",
        "'speaker_D': 'um one way'",
        "'speaker_D': 'to solve this problem is increase the number of outputs of the neural networks.'",
        "'speaker_D': 'Doing something like, um'",
        "'speaker_D': 'um phonemes within context and, well, basically context dependent phonemes.'",
        "'speaker_B': 'Maybe.'",
        "'speaker_B': 'I mean, I I think you could make the same argument, it'd be just as legitimate, for hybrid systems as well.'",
        "'speaker_D': 'Yeah but, we know that'",
        "'speaker_B': 'Right.'",
        "'speaker_B': 'And in fact, th things get better with context dependent versions.'",
        "'speaker_B': 'Right?'",
        "'speaker_D': 'Ye yeah but here it's something different.'",
        "'speaker_D': 'We want to have features uh well,'",
        "'speaker_B': 'Yeah.'",
        "'speaker_D': 'um.'",
        "'speaker_B': 'Yeah, but it's still true that what you're doing'",
        "'speaker_B': 'is you're ignoring you're you're coming up with something to represent, whether it's a distribution, probability distribution or features,'",
        "'speaker_B': 'you're coming up with a set of variables that are representing'",
        "'speaker_B': 'uh, things that vary w over context.'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'Uh, and you're putting it all together, ignoring the differences in context.'",
        "'speaker_B': 'That that's true for the hybrid system, it's true for a tandem system.'",
        "'speaker_B': 'So, for that reason, when you in in in a hybrid system, when you incorporate context one way or another, you do get better scores.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'OK?'",
        "'speaker_B': 'But I it's it's a big deal'",
        "'speaker_B': 'to get that.'",
        "'speaker_B': 'I I'm I'm sort of'",
        "'speaker_B': 'And once you the other thing is that once you represent start representing more and more context'",
        "'speaker_B': 'it is'",
        "'speaker_B': 'uh much more'",
        "'speaker_B': 'um specific'",
        "'speaker_B': 'to a particular task in language.'",
        "'speaker_B': 'So um'",
        "'speaker_B': 'Uh, the'",
        "'speaker_B': 'the acoustics associated with uh a particular context, for instance you may have some kinds of contexts that will never occur'",
        "'speaker_B': 'in one language and will occur frequently in the other, so the qu the issue of getting enough training'",
        "'speaker_B': 'for a particular kind of context becomes harder.'",
        "'speaker_B': 'We already actually don't have a huge amount of training data'",
        "'speaker_B': 'um'",
        "'speaker_D': 'Yeah, but'",
        "'speaker_D': 'mmm,'",
        "'speaker_D': 'I mean, the'",
        "'speaker_D': 'the way we we do it now is that we have a neural network and'",
        "'speaker_D': 'basically'",
        "'speaker_D': 'the net network is trained almost to give binary decisions.'",
        "'speaker_B': 'Right.'",
        "'speaker_D': 'And'",
        "'speaker_D': 'uh binary decisions about phonemes.'",
        "'speaker_D': 'Nnn'",
        "'speaker_D': 'Uh'",
        "'speaker_B': 'Almost.'",
        "'speaker_B': 'But I mean it it it does give a distribution.'",
        "'speaker_D': 'It's'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'It's and and it is true that if there's two phones that are very similar,'",
        "'speaker_B': 'that uh the i it may prefer one but it will give a reasonably high value to the other, too.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'Yeah, sure but'",
        "'speaker_D': 'uh'",
        "'speaker_D': 'So basically it's almost binary decisions and'",
        "'speaker_D': 'um the idea of using more classes is'",
        "'speaker_D': 'to'",
        "'speaker_D': 'get something that's less binary decisions.'",
        "'speaker_B': 'Oh no, but it would still be even more of a binary decision.'",
        "'speaker_B': 'It it'd be even more of one.'",
        "'speaker_B': 'Because then you would say that in that this phone in this context is a one, but the same phone in a slightly different context is a zero.'",
        "'speaker_D': 'But yeah, but'",
        "'speaker_B': 'That would be even even more distinct of a binary decision.'",
        "'speaker_B': 'I actually would have thought you'd wanna go the other way and have fewer classes.'",
        "'speaker_D': 'Yeah, but if'",
        "'speaker_B': 'Uh, I mean for instance, the the thing I was arguing for before, but again which I don't think we have time to try, is something in which you would modify the code so you could train to have several outputs on and use articulatory features'",
        "'speaker_D': 'Mmm.'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'cuz then that would that would go that would be much broader and cover many different situations.'",
        "'speaker_B': 'But if you go to very very fine categories, it's very binary.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_D': 'Yeah, but I think'",
        "'speaker_D': 'Yeah, perhaps you're right, but you have more classes so you you have more information in your features.'",
        "'speaker_D': 'So,'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_D': 'Um You have more information in the'",
        "'speaker_B': 'True.'",
        "'speaker_D': 'uh posteriors vector'",
        "'speaker_D': 'um which means that'",
        "'speaker_D': 'But still the information is relevant because it's it's information that helps to discriminate, if it's possible to be able to discriminate'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_D': 'among the phonemes in context.'",
        "'speaker_B': 'Well it's it's it's an interesting thought.'",
        "'speaker_D': 'But the'",
        "'speaker_B': 'I mean we we could disagree about it at length but the the real thing is if you're interested in it you'll probably try it and and we'll see.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'But but what I'm more concerned with now, as an operational level, is uh, you know, what do we do in four or five days?'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'Uh, and so we have to be concerned with'",
        "'speaker_B': 'Are we gonna look at any combinations of things, you know once the nets get retrained so you have this problem out of it.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'Um, are we going to look at multi band?'",
        "'speaker_B': 'Are we gonna look at combinations of things?'",
        "'speaker_B': 'Uh, what questions are we gonna ask, uh now that, I mean, we should probably turn shortly to thisOG note.'",
        "'speaker_B': 'Um, how are we going to combine with what they've been focusing on?'",
        "'speaker_B': 'Uh, Uh we haven't been doing any of theLD RASTA sort of thing.'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'And they, although they don't talk about it in this note, um,'",
        "'speaker_B': 'there's um, the issue of the'",
        "'speaker_B': 'um'",
        "'speaker_B': 'Mu law'",
        "'speaker_B': 'business uh versus the logarithm,'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'um,'",
        "'speaker_B': 'so.'",
        "'speaker_B': 'So what i what is going on right now?'",
        "'speaker_B': 'What's right you've got nets retraining, Are there is there are there anyHT trainings testings going on?'",
        "'speaker_D': 'N'",
        "'speaker_E': 'I I I'm trying the HTK with'",
        "'speaker_E': 'eh,'",
        "'speaker_E': 'PLP twelve on line delta delta and MSG filter together.'",
        "'speaker_B': 'The combination, I see.'",
        "'speaker_E': 'The combination, yeah.'",
        "'speaker_E': 'But I haven't result at this moment.'",
        "'speaker_B': 'MSG and and PLP.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_B': 'And is this with the revised on line normalization?'",
        "'speaker_E': 'Ye Uh, with the old older, yeah.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Old one.'",
        "'speaker_B': 'So it's using all the nets for that but again we have the hope that it'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'But We can'",
        "'speaker_B': 'We have the hope that it maybe it's not making too much difference, but but yeah.'",
        "'speaker_E': 'know soon.'",
        "'speaker_E': 'Maybe.'",
        "'speaker_E': 'I don't know.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Uh, OK.'",
        "'speaker_D': 'Uh so there is this combination, yeah.'",
        "'speaker_D': 'Working on combination obviously.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_D': 'Um, I will start work on multi band.'",
        "'speaker_D': 'And'",
        "'speaker_D': 'we'",
        "'speaker_D': 'plan to work also on the idea of using both'",
        "'speaker_D': 'features and net outputs.'",
        "'speaker_D': 'Um.'",
        "'speaker_D': 'And'",
        "'speaker_D': 'we think that'",
        "'speaker_D': 'with this approach perhaps'",
        "'speaker_D': 'we could reduce the number of outputs of the neural network.'",
        "'speaker_D': 'Um, So, get simpler networks, because we still have the features.'",
        "'speaker_D': 'So we have um'",
        "'speaker_D': 'come up with um'",
        "'speaker_D': 'different kind of'",
        "'speaker_D': 'broad phonetic categories.'",
        "'speaker_D': 'And we have Basically we have three types of broad phonetic classes.'",
        "'speaker_D': 'Well, something using place of articulation which which leads to nine, I think,'",
        "'speaker_D': 'broad classes.'",
        "'speaker_D': 'Uh, another which is based on manner, which is is also something like nine classes.'",
        "'speaker_D': 'And then, something that combine both, and we have'",
        "'speaker_D': 'twenty f twenty five?'",
        "'speaker_F': 'Twenty seven.'",
        "'speaker_D': 'Twenty seven broad classes.'",
        "'speaker_D': 'So like, uh, oh, I don't know,'",
        "'speaker_D': 'like back vowels, front vowels.'",
        "'speaker_B': 'So what you do um I just wanna understand so You have two net or three nets?'",
        "'speaker_D': 'Um'",
        "'speaker_B': 'Was this?'",
        "'speaker_B': 'How many how many nets do you have?'",
        "'speaker_D': 'For the moments we do not don't have nets, I mean,'",
        "'speaker_B': 'No nets.'",
        "'speaker_D': 'It's just Were we just changing the labels to retrain nets with fewer out outputs.'",
        "'speaker_E': 'Begin to work in this.'",
        "'speaker_E': 'We are.'",
        "'speaker_B': 'Right.'",
        "'speaker_D': 'And then'",
        "'speaker_B': 'But but I didn't understand'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'Uh.'",
        "'speaker_B': 'the software currently just has uh a allows for I think, the one one hot'",
        "'speaker_B': 'output.'",
        "'speaker_B': 'So you're having multiple nets and combining them, or?'",
        "'speaker_B': 'Uh, how are you how are you coming up with'",
        "'speaker_B': 'If you say'",
        "'speaker_B': 'uh'",
        "'speaker_B': 'If you have a place characteristic and a manner characteristic, how do you'",
        "'speaker_D': 'It'",
        "'speaker_A': 'I think they have'",
        "'speaker_D': 'It's the single net, yeah.'",
        "'speaker_A': 'one output.'",
        "'speaker_B': 'Oh, it's just one net.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_F': 'mm hmm'",
        "'speaker_D': 'It's one net with'",
        "'speaker_D': 'um'",
        "'speaker_D': 'twenty seven outputs if we have twenty seven classes, yeah.'",
        "'speaker_B': 'I see.'",
        "'speaker_B': 'I see, OK.'",
        "'speaker_D': 'So it's Well, it's basically a standard net with fewer classes.'",
        "'speaker_B': 'So you're sort of going the other way of what you were saying a bit ago instead of yeah.'",
        "'speaker_D': 'Yeah, but I think Yeah.'",
        "'speaker_F': 'But including the features.'",
        "'speaker_D': 'B b including the features, yeah.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_D': 'I don't think this will work alone.'",
        "'speaker_D': 'I think it will get worse because'",
        "'speaker_B': 'Uh huh.'",
        "'speaker_D': 'Well, I believe the effect that of of too'",
        "'speaker_D': 'reducing too much the information is'",
        "'speaker_D': 'basically basically what happens and'",
        "'speaker_B': 'But you think if you include that plus the other features,'",
        "'speaker_D': 'but'",
        "'speaker_D': 'Yeah, because there is perhaps one important thing that the net brings, and OGI show showed that, is'",
        "'speaker_D': 'the distinction between sp speech and silence'",
        "'speaker_D': 'Because these nets are trained on well controlled condition.'",
        "'speaker_D': 'I mean the labels are obtained on clean speech, and we add noise after.'",
        "'speaker_D': 'So this is one thing'",
        "'speaker_D': 'And'",
        "'speaker_D': 'But perhaps, something intermediary using also some broad classes could could bring so much more information.'",
        "'speaker_D': 'Uh.'",
        "'speaker_B': 'So so again then we have these broad classes and well, somewhat broad.'",
        "'speaker_B': 'I mean, it's twenty seven instead of sixty four, basically.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'And you have the original features.'",
        "'speaker_B': 'Which are PLP, or something.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'And then uh, just to remind me, all of that goes into'",
        "'speaker_B': 'uh, that all of that is transformed by uh, uh,K KL or something, or?'",
        "'speaker_D': 'There will probably be, yeah, one single KL to transform everything or'",
        "'speaker_E': 'Mu.'",
        "'speaker_B': 'Right.'",
        "'speaker_E': 'No transform the PLP and only'",
        "'speaker_D': 'uh, per'",
        "'speaker_E': 'transform the other'",
        "'speaker_E': 'I'm not sure.'",
        "'speaker_D': 'This is still something that yeah, we don't know'",
        "'speaker_B': 'Well no, I think'",
        "'speaker_B': 'I see.'",
        "'speaker_B': 'So there's a question of whether you would'",
        "'speaker_E': 'Two e'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Right.'",
        "'speaker_B': 'Whether you would transform together or just one.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_E': 'one.'",
        "'speaker_B': 'Might wanna try it both ways.'",
        "'speaker_B': 'But that's interesting.'",
        "'speaker_B': 'So that's something that you're you haven't trained yet but are preparing to train, and'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'Um Yeah, so I think Hynek will be here Monday.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'Monday or Tuesday.'",
        "'speaker_B': 'So'",
        "'speaker_D': 'Uh, yeah.'",
        "'speaker_B': 'So I think, you know, we need to choose the choose the experiments carefully,'",
        "'speaker_B': 'so we can get uh key key questions answered uh before then and'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'leave other ones aside even if it leaves incomplete tables'",
        "'speaker_B': 'someplace, uh'",
        "'speaker_B': 'uh, it's it's really time to'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'time to choose.'"
    ],
    "OGI voice activity detection (VAD) results": [
        "'speaker_B': 'Um, let me pass this out, by the way.'",
        "'speaker_B': 'Um'",
        "'speaker_B': 'These are'",
        "'speaker_B': 'Did did did I interrupt you?'",
        "'speaker_E': 'Yeah, I have one.'",
        "'speaker_B': 'Were there other things that you wanted to'",
        "'speaker_D': 'Uh, no.'",
        "'speaker_D': 'I don't think so.'",
        "'speaker_D': 'Yeah, I have one.'",
        "'speaker_G': 'Oh, thanks.'",
        "'speaker_B': 'Ah!'",
        "'speaker_E': 'We have one.'",
        "'speaker_B': 'OK.'",
        "'speaker_B': 'OK, we have lots of them.'",
        "'speaker_B': 'OK, so'",
        "'speaker_B': 'um,'",
        "'speaker_B': 'Something I asked So they're they're doing the the VAD I guess they mean voice activity detection So again, it's the silence So they've just trained up a net'",
        "'speaker_B': 'which has two outputs, I believe.'",
        "'speaker_B': 'Um'",
        "'speaker_B': 'I asked uh Hynek whether I haven't talked to Sunil I asked Hynek whether'",
        "'speaker_B': 'they compared that to just taking the nets we already had and summing up the probabilities.'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'Uh.'",
        "'speaker_B': 'To get the speech voice activity detection, or else just using the silence, if there's only one silence output.'",
        "'speaker_B': 'Um'",
        "'speaker_B': 'And, he didn't think they had, um.'",
        "'speaker_B': 'But on the other hand, maybe they can get by with a smaller net and maybe sometimes you don't run the other, maybe there's a computational advantage to having a separate net, anyway.'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'So um'",
        "'speaker_B': 'Their uh the results look pretty good.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Um, I mean, not uniformly.'",
        "'speaker_B': 'I mean, there's a an example or two that you can find, where it made it slightly worse, but uh in in all but a couple'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'examples.'",
        "'speaker_B': 'Uh.'",
        "'speaker_E': 'But they have a question of the result.'",
        "'speaker_E': 'Um how'",
        "'speaker_E': 'are trained the'",
        "'speaker_E': 'the LDA filter?'",
        "'speaker_E': 'How obtained the LDA filter?'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'I I'm sorry.'",
        "'speaker_B': 'I don't understand your question.'",
        "'speaker_E': 'Yes, um the LDA filter'",
        "'speaker_E': 'needs some training set'",
        "'speaker_E': 'to obtain the filter.'",
        "'speaker_E': 'Maybe'",
        "'speaker_E': 'I don't know exactly how'",
        "'speaker_B': 'It's on training.'",
        "'speaker_E': 'they are obtained.'",
        "'speaker_E': 'Training, with the training test of each'",
        "'speaker_E': 'You understand me?'",
        "'speaker_B': 'No.'",
        "'speaker_E': 'Yeah, uh for example, LDA filter'",
        "'speaker_E': 'need a set'",
        "'speaker_E': 'of'",
        "'speaker_E': 'a set of training to obtain the filter.'",
        "'speaker_B': 'Yes.'",
        "'speaker_E': 'And maybe for the Italian, for the TD TE on for'",
        "'speaker_E': 'Finnish,'",
        "'speaker_E': 'these filter are are obtained with their own'",
        "'speaker_E': 'training set.'",
        "'speaker_B': 'Yes, I don't know.'",
        "'speaker_B': 'That's that's so that's a that's a very good question, then now that it I understand it.'",
        "'speaker_B': 'It's\" yeah, where does the LDA come from?'",
        "'speaker_B': 'In the In earlier experiments, they had taken LDA from a completely different database, right?'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'Yeah, because maybe it the same situation that the neural network training with their own'",
        "'speaker_D': 'Mmm.'",
        "'speaker_E': 'set.'",
        "'speaker_B': 'So that's a good question.'",
        "'speaker_B': 'Where does it come from?'",
        "'speaker_B': 'Yeah, I don't know.'",
        "'speaker_B': 'Um, but uh to tell you the truth, I wasn't actually looking at the LDA so much when I I was looking at it I was mostly thinking about the the VAD.'",
        "'speaker_B': 'And um, it ap it ap Oh what does what does ASP?'",
        "'speaker_B': 'Oh that's'",
        "'speaker_D': 'The features, yeah.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_E': 'I don't understand also what is'",
        "'speaker_B': 'It says baseline ASP\".'",
        "'speaker_E': 'what is the difference between ASP and uh baseline over?'",
        "'speaker_D': 'Yeah, I don't know.'",
        "'speaker_C': 'ASP.'",
        "'speaker_E': 'This is'",
        "'speaker_C': 'Oh.'",
        "'speaker_B': 'Anybody know any'",
        "'speaker_C': 'There it is.'",
        "'speaker_B': 'Um'",
        "'speaker_B': 'Cuz there's baseline Aurora\" above it.'",
        "'speaker_C': 'Mm hmm.'",
        "'speaker_B': 'And it's'",
        "'speaker_B': 'This is mostly better than baseline, although in some cases it's a little worse, in a couple cases.'",
        "'speaker_C': 'Well, it says baseline ASP is twenty three mill'",
        "'speaker_E': 'Yeah.'",
        "'speaker_C': 'minus thirteen.'",
        "'speaker_B': 'Yeah, it says what it is.'",
        "'speaker_B': 'But I don't how that's different from'",
        "'speaker_C': 'From the baseline.'",
        "'speaker_C': 'OK.'",
        "'speaker_B': 'I think this was I think this is the same point we were at when when we were up in Oregon.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_D': 'I think I think it's theC zero usingC zero instead of log energy.'",
        "'speaker_E': 'Ah, OK, mm hmm.'",
        "'speaker_D': 'Yeah, it's this.'",
        "'speaker_E': 'yeah.'",
        "'speaker_B': 'Oh.'",
        "'speaker_B': 'OK.'",
        "'speaker_D': 'It should be that, yeah.'",
        "'speaker_D': 'Because'",
        "'speaker_B': 'Shouldn't it be'",
        "'speaker_A': 'They s they say in here that the VAD is not used as an additional feature.'",
        "'speaker_A': 'Does does anybody know how they're using it?'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'So so what they're doing here is, i if you look down at the block diagram,'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'um,'",
        "'speaker_A': 'But that'",
        "'speaker_B': 'they estimate they get a they get an estimate of whether it's speech or silence, and then they have a median filter of it.'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_B': 'And so um, basically they're trying to find stretches.'",
        "'speaker_B': 'The median filter is enforcing a i it having some continuity.'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_B': 'You find stretches where the combination of the frame wise VAD and the the median filter say that there's a stretch of silence.'",
        "'speaker_B': 'And then it's going through and just throwing the data away.'",
        "'speaker_C': 'Hmm.'",
        "'speaker_B': 'Right?'",
        "'speaker_B': 'So um'",
        "'speaker_A': 'So it's it's'",
        "'speaker_A': 'I don't understand.'",
        "'speaker_A': 'You mean it's throwing out frames?'",
        "'speaker_A': 'Before'",
        "'speaker_B': 'It's throwing out chunks of frames, yeah.'",
        "'speaker_B': 'There's the the median filter is enforcing that it's not gonna be single cases of frames, or isolated frames.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_B': 'So it's throwing out frames and the thing is um, what I don't understand is how they're doing this withHT.'",
        "'speaker_A': 'Yeah, that's what I was just gonna ask.'",
        "'speaker_B': 'This is'",
        "'speaker_A': 'How can you just throw out frames?'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'Well, you you can, right?'",
        "'speaker_D': 'i'",
        "'speaker_B': 'I mean y you you'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'it stretches again.'",
        "'speaker_B': 'For single frames I think it would be pretty hard.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_B': 'But if you say speech starts here, speech ends there.'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_B': 'Right?'",
        "'speaker_C': 'Huh.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_D': 'Yeah, you can basically remove the the frames from the feature feature files.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'Yeah, so I mean in the i i in the in the decoding, you're saying that we're gonna decode from here to here.'",
        "'speaker_D': 'I t'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_B': 'I think they're they're they're treating it, you know, like uh well, it's not isolated word, but but connected, you know, the the'",
        "'speaker_A': 'In the text they say that this this is a tentative block diagram of a possible configuration we could think of.'",
        "'speaker_A': 'So that sort of sounds like they're not doing that yet.'",
        "'speaker_B': 'Well.'",
        "'speaker_B': 'No they they have numbers though, right?'",
        "'speaker_B': 'So I think they're they're doing something like that.'",
        "'speaker_B': 'I think that they're they're'",
        "'speaker_B': 'I think what I mean by tha that is they're trying to come up with a block diagram that's plausible for the standard.'",
        "'speaker_B': 'In other words, it's uh I mean from the point of view of of uh reducing the number of bits you have to transmit it's not a bad idea to detect silence anyway.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_B': 'Um.'",
        "'speaker_A': 'I'm just wondering what exactly did they do up in this table if it wasn't this.'",
        "'speaker_B': 'But it's the thing is it's that that that's I I'",
        "'speaker_B': 'Certainly it would be tricky about it intrans in transmitting voice, uh uh for listening to, is that these kinds of things'",
        "'speaker_B': 'uh cut'",
        "'speaker_B': 'speech off a lot.'",
        "'speaker_B': 'Right?'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_B': 'And so um'",
        "'speaker_A': 'Plus it's gonna introduce delays.'",
        "'speaker_B': 'It does introduce delays but they're claiming that it's it's within the the boundaries of it.'",
        "'speaker_A': 'Mmm.'",
        "'speaker_B': 'And the LDA introduces delays, and b what he's suggesting this here is a parallel path so that it doesn't introduce'",
        "'speaker_B': 'uh, any more delay.'",
        "'speaker_B': 'I it introduces two hundred milliseconds of delay but at the same time the LDA down here'",
        "'speaker_B': 'I don't know Wh what's the difference between TLDA and SLDA?'",
        "'speaker_C': 'Temporal and spectral.'",
        "'speaker_B': 'Ah, thank you.'",
        "'speaker_E': 'Temporal LDA.'",
        "'speaker_B': 'Yeah, you would know that.'",
        "'speaker_C': 'Yeah'",
        "'speaker_B': 'So um.'",
        "'speaker_B': 'The temporal LDA does in fact include the same'",
        "'speaker_B': 'so that I think he well, by by saying this is a b a tentative block di diagram I think means'",
        "'speaker_B': 'if you construct it this way, this this delay would work in that way and then it'd be OK.'",
        "'speaker_A': 'Ah.'",
        "'speaker_B': 'They they clearly did actually remove'",
        "'speaker_B': 'silent sections in order because they'",
        "'speaker_B': 'got these'",
        "'speaker_B': 'word error rate results.'",
        "'speaker_B': 'So um'",
        "'speaker_B': 'I think that it's it's nice to do that in this because in fact, it's gonna give a better word error result and therefore will help within an evaluation.'",
        "'speaker_B': 'Whereas to whether this would actually be in a final standard, I don't know.'",
        "'speaker_B': 'Um.'",
        "'speaker_B': 'Uh, as you know, part of the problem with evaluation right now is that the word models are pretty bad and nobody wants has has approached improving them.'",
        "'speaker_B': 'So it's possible that a lot of the problems with so many insertions and so forth would go away if they were better word models to begin with.'",
        "'speaker_B': 'So'",
        "'speaker_B': 'this might just be a temporary thing.'",
        "'speaker_B': 'But'",
        "'speaker_B': 'But, on the other hand, and maybe maybe it's a decent idea.'"
    ],
    "collaboration with OGI(?), parameter selection, allocation": [
        "'speaker_B': 'So um'",
        "'speaker_B': 'The question we're gonna wanna go through next week when Hynek shows up I guess is given that we've been if you look at what we've been trying,'",
        "'speaker_B': 'we're uh looking at uh,'",
        "'speaker_B': 'by then I guess, combinations of features and multi band'",
        "'speaker_B': 'Uh, and we've been looking at cross language, cross task issues.'",
        "'speaker_B': 'And they've been not so much looking at'",
        "'speaker_B': 'the cross task uh multiple language issues.'",
        "'speaker_B': 'But they've been looking at uh at these issues.'",
        "'speaker_B': 'At the'",
        "'speaker_B': 'on line normalization and the uh'",
        "'speaker_B': 'voice activity detection.'",
        "'speaker_B': 'And I guess when he comes here we're gonna have to start deciding about um what do we choose from what we've looked at'",
        "'speaker_B': 'to um blend with some group of things in what they've looked at And once we choose that, how do we split up the effort?'",
        "'speaker_B': 'Uh, because we still have even once we choose,'",
        "'speaker_B': 'we've still got uh another'",
        "'speaker_B': 'month or so, I mean there's holidays in the way, but but uh I think the evaluation data comes January thirty first so there's still a fair amount of time'",
        "'speaker_B': 'to do things together it's just that they probably should be somewhat more coherent between the two sites in that that amount of time.'",
        "'speaker_A': 'When they removed the silence frames, did they insert some kind of a marker so that the recognizer knows it's knows when it's time to back trace or something?'",
        "'speaker_B': 'Well, see they, I I think they're'",
        "'speaker_B': 'Um.'",
        "'speaker_B': 'I don't know the'",
        "'speaker_B': 'the specifics of how they're doing it.'",
        "'speaker_B': 'They're'",
        "'speaker_B': 'they're getting around the way the recognizer works because they're not allowed to'",
        "'speaker_B': 'um, change the scripts'",
        "'speaker_A': 'Oh, right.'",
        "'speaker_B': 'for the recognizer, I believe.'",
        "'speaker_B': 'So.'",
        "'speaker_A': 'Maybe they're just inserting some nummy frames or something?'",
        "'speaker_B': 'Uh.'",
        "'speaker_B': 'Uh, you know that's what I had thought.'",
        "'speaker_B': 'But I don't I don't think they are.'",
        "'speaker_A': 'Hmm.'",
        "'speaker_B': 'I mean that's sort of what the way I had imagined would happen is that on the other side, yeah you p put some low level noise or something.'",
        "'speaker_B': 'Probably don't want all zeros.'",
        "'speaker_B': 'Most recognizers don't like zeros but'",
        "'speaker_A': 'Hmm.'",
        "'speaker_B': 'but you know, put some epsilon in or some rand sorry epsilon random variable in or something.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_A': 'Some constant vector.'",
        "'speaker_A': 'I mean i w'",
        "'speaker_B': 'Maybe not a constant but it doesn't, uh'",
        "'speaker_A': 'Or something'",
        "'speaker_B': 'don't like to divide by the variance of that, but I mean it's'",
        "'speaker_A': 'That's'",
        "'speaker_A': 'right.'",
        "'speaker_A': 'But something that what I mean is something that is very distinguishable from speech.'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_A': 'So that the'",
        "'speaker_A': 'the silence model in HTK will always pick it up.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'So I I that's what I thought they would do.'",
        "'speaker_B': 'or else, uh uh maybe there is some indicator to tell it to start and stop, I don't know.'",
        "'speaker_B': 'But whatever they did, I mean they have to play within the rules of this specific evaluation.'",
        "'speaker_A': 'Hmm.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_B': 'We c we can find out.'",
        "'speaker_A': 'Cuz you gotta do something.'",
        "'speaker_A': 'Otherwise, if it's just a bunch of speech, stuck together'",
        "'speaker_B': 'No they're'",
        "'speaker_A': 'Yeah.'",
        "'speaker_B': 'It would do badly and it didn't so badly, right?'",
        "'speaker_A': 'Yeah, right.'",
        "'speaker_B': 'So they did something.'",
        "'speaker_A': 'Yeah, yeah.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'Uh.'",
        "'speaker_B': 'So, OK, So I think'",
        "'speaker_B': 'this brings me up to date a bit.'",
        "'speaker_B': 'It hopefully brings other people up to date a bit.'",
        "'speaker_B': 'And um'",
        "'speaker_B': 'Um I think'",
        "'speaker_B': 'Uh, I wanna look at these numbers off line a little bit and think about it and and talk with everybody uh, outside of this meeting.'",
        "'speaker_B': 'Um, but uh'",
        "'speaker_B': 'No I mean it sounds like I mean there there there are the usual number of of little little problems and bugs and so forth but it sounds like they're getting ironed out.'",
        "'speaker_B': 'And now we're'",
        "'speaker_B': 'seem to be kind of in a position to actually uh, look at stuff and and and compare things.'",
        "'speaker_B': 'So I think that's that's pretty good.'",
        "'speaker_B': 'Um I don't know what the'",
        "'speaker_B': 'One of the things I wonder about,'",
        "'speaker_B': 'coming back to the first results you talked about, is is how much, uh things could be helped'",
        "'speaker_B': 'by more parameters.'",
        "'speaker_B': 'And uh And uh how many more parameters we can afford to have,'",
        "'speaker_B': 'in terms of the uh computational limits.'",
        "'speaker_B': 'Because anyway when we go to'",
        "'speaker_B': 'twice as much data'",
        "'speaker_B': 'and have the same number of parameters, particularly when it's twice as much data and it's quite diverse,'",
        "'speaker_B': 'um, I wonder if having twice as many parameters would help.'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'Uh, just have a bigger hidden layer.'",
        "'speaker_B': 'Uh'",
        "'speaker_B': 'But'",
        "'speaker_B': 'I doubt it would help by forty per cent.'",
        "'speaker_B': 'But'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'but uh'",
        "'speaker_B': 'Just curious.'"
    ],
    "disk resources, servers": [
        "'speaker_B': 'How are we doing on the'",
        "'speaker_B': 'resources?'",
        "'speaker_B': 'Disk, and'",
        "'speaker_D': 'I think we're alright, um, not'",
        "'speaker_B': 'OK.'",
        "'speaker_D': 'much problems with that.'",
        "'speaker_B': 'Computation?'",
        "'speaker_D': 'It's OK.'",
        "'speaker_D': 'Well this table took uh'",
        "'speaker_B': 'We'",
        "'speaker_D': 'more than five days to get back.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'Yeah, well.'",
        "'speaker_D': 'But Yeah.'",
        "'speaker_B': 'Are were you folks using Gin?'",
        "'speaker_B': 'That's a that just died, you know?'",
        "'speaker_D': 'Mmm, no.'",
        "'speaker_D': 'You were using Gin perhaps, yeah?'",
        "'speaker_D': 'No.'",
        "'speaker_E': 'No.'",
        "'speaker_B': 'No?'",
        "'speaker_B': 'Oh, that's good.'",
        "'speaker_F': 'It just died.'",
        "'speaker_B': 'OK.'",
        "'speaker_B': 'Yeah, we're gonna get a replacement server that'll be a faster server, actually.'",
        "'speaker_E': 'Yes.'",
        "'speaker_D': 'Hmm.'",
        "'speaker_B': 'That'll be It's a'",
        "'speaker_B': 'seven hundred fifty megahertz uh SUN'",
        "'speaker_C': 'Tonic.'",
        "'speaker_B': 'uh'",
        "'speaker_B': 'But it won't be installed for a little while.'",
        "'speaker_D': 'Mm hmm.'",
        "'speaker_B': 'U Go ahead.'",
        "'speaker_G': 'Do we'",
        "'speaker_G': 'Do we have that big new IBM machine the, I think in th'",
        "'speaker_B': 'We have the little tiny IBM machine'",
        "'speaker_B': 'that might someday grow up to be a big IBM machine.'",
        "'speaker_B': 'It's got s slots for eight,'",
        "'speaker_B': 'uh IBM was donating five, I think we only got two so far,'",
        "'speaker_B': 'processors.'",
        "'speaker_B': 'We had originally hoped we were getting eight hundred megahertz processors.'",
        "'speaker_B': 'They ended up being five fifty.'",
        "'speaker_B': 'So instead of having eight processors that were eight hundred megahertz, we ended up with two that are five hundred and fifty megahertz.'",
        "'speaker_B': 'And more are supposed to come soon and there's only a moderate amount of dat of memory.'",
        "'speaker_B': 'So I don't think'",
        "'speaker_B': 'anybody has been sufficiently'",
        "'speaker_B': 'excited by it to'",
        "'speaker_B': 'spend much time'",
        "'speaker_B': 'uh'",
        "'speaker_B': 'with it, but uh'",
        "'speaker_B': 'Hopefully, they'll get us some more'",
        "'speaker_B': 'parts, soon and'",
        "'speaker_B': 'Uh, yeah, I think that'll be once we get it populated, that'll be a nice machine.'",
        "'speaker_B': 'I mean we will ultimately get eight processors in there.'",
        "'speaker_B': 'And uh and uh a nice amount of memory.'",
        "'speaker_B': 'Uh so it'll be a pr pretty fast Linux machine.'",
        "'speaker_G': 'And if we can do things on Linux, some of the machines we have going already, like Swede?'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_G': 'Um'",
        "'speaker_G': 'It seems pretty fast.'",
        "'speaker_B': 'Mm hmm.'",
        "'speaker_G': 'But I think Fudge is pretty fast too.'",
        "'speaker_B': 'Yeah, I mean you can check with uh Dave Johnson.'",
        "'speaker_B': 'I mean, it it's I think the machine is just sitting there.'",
        "'speaker_B': 'And it does have two processors, you know and'",
        "'speaker_B': 'Somebody could do'",
        "'speaker_B': 'you know, uh, check out uh the multi threading libraries.'",
        "'speaker_B': 'And I mean i it's possible that the I'",
        "'speaker_B': 'mean, I guess the prudent thing to do would be for somebody to do the work on'",
        "'speaker_B': 'on getting our code running on that machine with two processors even though there aren't five or eight.'",
        "'speaker_B': 'There's there's there's gonna be debugging hassles'",
        "'speaker_B': 'and then we'd be set for when we did have five or eight, to have it really be useful.'",
        "'speaker_B': 'But.'",
        "'speaker_B': 'Notice how I said somebody and'",
        "'speaker_B': 'turned my head your direction.'",
        "'speaker_B': 'That's one thing you don't get in these recordings.'",
        "'speaker_B': 'You don't get the'",
        "'speaker_B': 'don't get the visuals but'",
        "'speaker_G': 'I is it um'",
        "'speaker_G': 'mostly um the neural network trainings that are'",
        "'speaker_G': 'um slowing us down or the HTK runs that are slowing us down?'",
        "'speaker_B': 'Uh, I think yes.'",
        "'speaker_B': 'Uh,'",
        "'speaker_B': 'Isn't that right?'",
        "'speaker_B': 'I mean I think you're you're sort of held up by both, right?'",
        "'speaker_B': 'If the if the neural net trainings were a hundred times faster'",
        "'speaker_B': 'you still wouldn't be anything running through these a hundred times faster because you'd'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'be stuck by the HTK trainings, right?'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'But if the HTK I mean I think they're both It sounded like they were roughly equal?'",
        "'speaker_B': 'Is that about right?'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_G': 'Because, um I think that'll be running Linux, and Sw Swede and Fudge are already running Linux so,'",
        "'speaker_G': 'um I could try to get um the train the neural network trainings or the HTK stuff running under Linux, and to start with I'm wondering which one I should pick first.'",
        "'speaker_B': 'Uh, probably the neural net cuz it's probably it it's it's um'",
        "'speaker_B': 'Well, I I don't know.'",
        "'speaker_B': 'They both'",
        "'speaker_B': 'HTK we use for'",
        "'speaker_B': 'um this Aurora stuff Um'",
        "'speaker_B': 'Um, I think It's not clear yet what we're gonna use'",
        "'speaker_B': 'for trainings uh Well,'",
        "'speaker_B': 'there's the trainings uh is it the training that takes the time, or the decoding?'",
        "'speaker_B': 'Uh, is it about equal between the two?'",
        "'speaker_B': 'For for Aurora?'",
        "'speaker_D': 'For HTK?'",
        "'speaker_B': 'For Yeah.'",
        "'speaker_B': 'For the Aurora?'",
        "'speaker_D': 'Uh Training is longer.'",
        "'speaker_B': 'OK.'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'OK.'",
        "'speaker_B': 'Well, I don't know how we can'",
        "'speaker_B': 'I don't know how to'",
        "'speaker_B': 'Do we have HTK source?'",
        "'speaker_B': 'Is that'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'Yeah.'",
        "'speaker_B': 'You would think that would fairly trivially'",
        "'speaker_B': 'the training would, anyway, th the testing uh I don't I don't'",
        "'speaker_B': 'think would parallelize all that well.'",
        "'speaker_B': 'But I think that you could'",
        "'speaker_B': 'certainly do d um,'",
        "'speaker_B': 'distributed, sort of'",
        "'speaker_B': 'Ah, no, it's the each individual sentence'",
        "'speaker_B': 'is pretty tricky to parallelize.'",
        "'speaker_B': 'But you could split up the sentences in a test set.'",
        "'speaker_A': 'They have a they have a thing for doing that and th they have for awhile, inHT.'",
        "'speaker_B': 'Yeah?'",
        "'speaker_A': 'And you can parallelize the training.'",
        "'speaker_A': 'And run it on several machines and it just basically keeps counts.'",
        "'speaker_B': 'Aha!'",
        "'speaker_A': 'And there's something a final thing that you run and it accumulates all the counts together.'",
        "'speaker_D': 'Mmm.'",
        "'speaker_B': 'I see.'",
        "'speaker_A': 'I don't what their scripts are set up to do for the Aurora stuff, but'",
        "'speaker_D': 'Yeah.'",
        "'speaker_B': 'Something that we haven't really settled on yet is other than this Aurora stuff, uh what do we do, large vocabulary'",
        "'speaker_B': 'training slash testing'",
        "'speaker_B': 'for uh tandem systems.'",
        "'speaker_B': 'Cuz we hadn't really done much with tandem systems for larger stuff.'",
        "'speaker_B': 'Cuz we had this one collaboration with CMU and we used SPHINX.'",
        "'speaker_B': 'Uh, we're also gonna be collaborating with SRI and we have their have theirs.'",
        "'speaker_B': 'Um So'",
        "'speaker_B': 'I don't know'",
        "'speaker_B': 'Um.'",
        "'speaker_B': 'So I I think the the advantage of going with the neural net thing is that we're gonna use the neural net trainings, no matter what, for a lot of the things we're doing,'",
        "'speaker_G': 'OK.'",
        "'speaker_B': 'whereas, w exactly which'",
        "'speaker_B': 'HMM Gaussian based HMM thing we use is gonna depend'",
        "'speaker_B': 'uh'"
    ],
    "closing": [
        "'speaker_B': 'So with that, maybe we should uh'",
        "'speaker_B': 'go to our'",
        "'speaker_B': 'digit recitation task.'",
        "'speaker_B': 'And, it's about eleven fifty.'",
        "'speaker_B': 'Canned.'",
        "'speaker_B': 'Uh, I can I can start over here.'",
        "'speaker_B': 'Great, uh, could you give Adam a call.'",
        "'speaker_B': 'Tell him to'",
        "'speaker_F': 'Oh.'",
        "'speaker_B': 'He's at two nine seven seven.'",
        "'speaker_B': 'OK.'",
        "'speaker_B': 'I think we can'",
        "'speaker_B': 'You know Herve's coming tomorrow, right?'",
        "'speaker_B': 'Herve will be giving a talk, yeah, talk at eleven.'",
        "'speaker_B': 'Did uh, did everybody sign these consent'",
        "'speaker_B': 'Er everybody'",
        "'speaker_B': 'Has everyone signed a consent form before, on previous meetings?'",
        "'speaker_B': 'You don't have to do it again each time'",
        "'speaker_B': 'Yes.'",
        "'speaker_B': 'microphones off'"
    ],
    "DARPA demo": []
}