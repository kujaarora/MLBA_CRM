{
    "update from speaker_A, morning meeting report": [
        "'speaker_C': 'So uh'",
        "'speaker_C': 'Wanna talk a little bit about what we were talking about this morning?'",
        "'speaker_A': 'Oh!'",
        "'speaker_C': 'Just briefly, or'",
        "'speaker_A': 'um uh Yeah.'",
        "'speaker_C': 'Or anything else?'",
        "'speaker_A': 'So.'",
        "'speaker_A': 'I I guess some of the progress, I I've been getting a getting my committee members for the quals.'",
        "'speaker_A': 'And um so far I have Morgan and Hynek,'",
        "'speaker_A': 'Mike Jordan, and I asked John Ohala and he agreed.'",
        "'speaker_E': 'Cool.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_A': 'So I'm I I just need to ask um Malek.'",
        "'speaker_A': 'One more.'",
        "'speaker_A': 'Um.'",
        "'speaker_A': 'Tsk.'",
        "'speaker_A': 'Then uh I talked a little bit about um continuing with these dynamic ev um acoustic events,'",
        "'speaker_A': 'and um'",
        "'speaker_A': 'we're we're we're'",
        "'speaker_A': 'thinking about a way to test the completeness of'",
        "'speaker_A': 'a a set of um dynamic uh events.'",
        "'speaker_A': 'Uh, completeness in the in the sense that um if we if we pick theseX number of acoustic events, do they provide sufficient coverage for the phones that we're trying to recognize or or the f the words that we're gonna try to recognize later on.'",
        "'speaker_A': 'And so Morgan and I were uh discussing um s uh s'",
        "'speaker_A': 'a form of a cheating experiment where we get'",
        "'speaker_A': 'um we have uh'",
        "'speaker_A': 'um'",
        "'speaker_A': 'a chosen set of features, or acoustic events, and we train up a hybrid um system to do phone recognition on TIMIT.'",
        "'speaker_A': 'So i i the idea is if we get good phone recognition results,'",
        "'speaker_A': 'using um these set of acoustic events, then um that that says that these acoustic events are g sufficient to cover a set of phones, at least found in TIMIT.'",
        "'speaker_A': 'Um so i it would be a'",
        "'speaker_A': 'a measure of are we on the right track with with the the choices of our acoustic events\".'",
        "'speaker_A': 'Um, So that's going on.'",
        "'speaker_A': 'And'",
        "'speaker_A': 'also, just uh working on my uh final project for Jordan's class, uh which is'",
        "'speaker_C': 'Actually, let me Hold that thought.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_C': 'Let me back up while we're still on it.'",
        "'speaker_A': 'OK, sure.'",
        "'speaker_C': 'The the other thing I was suggesting, though, is that given that you're talking about binary features,'",
        "'speaker_C': 'uh, maybe the first thing to do is just to count'",
        "'speaker_C': 'and uh count co occurrences and get probabilities for a discrete HMM'",
        "'speaker_C': 'cuz that'd be pretty simple because it's just Say, if you had ten ten events,'",
        "'speaker_C': 'uh that you were counting, uh each frame'",
        "'speaker_C': 'would only have a thousand possible values for these ten bits,'",
        "'speaker_C': 'and uh so you could make a table that would say, if you had thirty nine phone categories,'",
        "'speaker_C': 'that would be a thousand by thirty nine, and just count the co occurrences and divide them by the'",
        "'speaker_C': 'the uh uh uh occ uh'",
        "'speaker_C': 'count the co occurrences between the event and the phone'",
        "'speaker_C': 'and divide them by the number of occurrences of the phone, and that would give you the likelihood of the'",
        "'speaker_C': 'of the event given the phone.'",
        "'speaker_C': 'And um then just use that in a very simple HMM and uh'",
        "'speaker_C': 'you could uh do phone recognition then and uh wouldn't have any of the issues of the uh training of the net or I'",
        "'speaker_C': 'mean, it'd be on the simple side, but'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'uh'",
        "'speaker_C': 'um'",
        "'speaker_C': 'you know, if uh uh the example I was giving was that if if you had um'",
        "'speaker_C': 'onset of voicing and and end of voicing as being two kinds of events,'",
        "'speaker_C': 'then if you had those a all marked correctly, and you counted co occurrences, you should get it completely right.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'So.'",
        "'speaker_C': 'um But you'd get all the other distinctions, you know, randomly wrong.'",
        "'speaker_C': 'I mean there'd be nothing to tell you that.'",
        "'speaker_C': 'So um'",
        "'speaker_C': 'uh'",
        "'speaker_C': 'If you just do this by counting, then you should be able to find out in a pretty straightforward way whether you have a sufficient uh set of events to to do the kind of level of'",
        "'speaker_C': 'of uh classification of phones that you'd like.'",
        "'speaker_C': 'So that was that was the idea.'",
        "'speaker_C': 'And then the other thing that we were discussing was was um'",
        "'speaker_C': 'OK, how do you get the your training data.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'Cuz uh the'",
        "'speaker_C': 'Switchboard transcription project uh uh you know was half a dozen people, or so working off and on over a couple years, and'",
        "'speaker_C': 'uh similar similar amount of data to what you're talking about with TIMIT training.'",
        "'speaker_C': 'So,'",
        "'speaker_C': 'it seems to me that the only reasonable starting point is'",
        "'speaker_C': 'uh to automatically translate the uh'",
        "'speaker_C': 'current TIMIT markings into the markings you want.'",
        "'speaker_C': 'And uh'",
        "'speaker_C': 'it won't have the kind of characteristic that you'd like, of'",
        "'speaker_C': 'catching'",
        "'speaker_C': 'funny kind of things that maybe aren't'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'there from these automatic markings, but but uh'",
        "'speaker_C': 'it's uh'",
        "'speaker_E': 'It's probably a good place to start.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_C': 'Yeah and a short short amount of time, just to'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'again, just to see if that information is sufficient'",
        "'speaker_C': 'to uh determine the phones.'",
        "'speaker_E': 'Hmm.'",
        "'speaker_C': 'So.'",
        "'speaker_E': 'Yeah, you could even then to to get an idea about'",
        "'speaker_E': 'how different it is, you could maybe take some subset and'",
        "'speaker_E': 'you know, go through a few sentences, mark them by hand and then see how different it is'",
        "'speaker_E': 'from'",
        "'speaker_E': 'you know, the canonical ones, just to get an idea a rough idea of'",
        "'speaker_C': 'Right.'",
        "'speaker_E': 'h if it really even makes a difference.'",
        "'speaker_C': 'You can get a little feeling for it that way, yeah that is probably right.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_C': 'I mean uh my my guess would be that this is since TIMIT's'",
        "'speaker_C': 'read speech that this would be less of a big deal, if you went and looked at spontaneous speech it'd be more more of one.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_E': 'Right.'",
        "'speaker_E': 'Right.'",
        "'speaker_C': 'And the other thing would be, say, if you had these ten events, you'd wanna see, well what if you took two events or four events or ten events or t and you know, and and hopefully there should be some point at which'",
        "'speaker_C': 'having more information doesn't tell you'",
        "'speaker_C': 'really all that much more about what the phones are.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_E': 'You could define'",
        "'speaker_E': 'other events as being sequences of these events'",
        "'speaker_E': 'too.'",
        "'speaker_C': 'Uh, you could, but the thing is, what he's talking about here is a uh a translation to'",
        "'speaker_C': 'a per frame feature vector,'",
        "'speaker_C': 'so there's no sequence in that, I think.'",
        "'speaker_C': 'I think it's just a'",
        "'speaker_E': 'Unless you did like a second pass over it or something after you've got your'",
        "'speaker_C': 'Yeah, but we're just talking about something simple here, yeah, to see if'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'Yeah, yeah.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'I'm adding complexity.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_C': 'Just You know.'",
        "'speaker_C': 'The idea is with a with a very simple statistical structure, could you could you uh'",
        "'speaker_E': 'Yeah.'",
        "'speaker_C': 'at least verify that you've chosen features that'",
        "'speaker_C': 'are sufficient.'",
        "'speaker_C': 'OK, and you were saying something starting to say something else about your your class project, or?'",
        "'speaker_A': 'Oh.'",
        "'speaker_A': 'Yeah th Um.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_A': 'So for my class project I'm'",
        "'speaker_A': 'um'",
        "'speaker_A': 'I'm'",
        "'speaker_A': 'tinkering with uh support vector machines?'",
        "'speaker_A': 'something that we learned in class, and uh um basically just another method for doing classification.'",
        "'speaker_A': 'And so I'm gonna apply that to um compare it with the results by um King and Taylor who did um these um using recurrent neural nets, they recognized um'",
        "'speaker_A': 'a set of phonological features um and made a mapping from the MFCC's to these phonological features, so I'm gonna do a similar thing with with support vector machines and see if'",
        "'speaker_E': 'So what's the advantage of support vector machines?'",
        "'speaker_E': 'What'",
        "'speaker_A': 'Um.'",
        "'speaker_A': 'So, support vector machines are are good with dealing with a less amount of data'",
        "'speaker_E': 'Hmm.'",
        "'speaker_A': 'and um so if you if you give it less data it still does a reasonable job in learning the the patterns.'",
        "'speaker_E': 'Hmm.'",
        "'speaker_A': 'Um and um'",
        "'speaker_C': 'I guess it yeah, they're sort of succinct, and and they'",
        "'speaker_A': 'Yeah.'",
        "'speaker_C': 'uh'",
        "'speaker_E': 'Does there some kind of a distance metric that they use or how do they'",
        "'speaker_E': 'for cla what do they do for classification?'",
        "'speaker_A': 'Um.'",
        "'speaker_A': 'Right.'",
        "'speaker_A': 'So, the the simple idea behind a support vector machine is um, you have'",
        "'speaker_A': 'you have this feature space, right?'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_A': 'and then it finds the optimal separating'",
        "'speaker_A': 'plane,'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_A': 'um between these two different um classes,'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_A': 'and um'",
        "'speaker_A': 'and so'",
        "'speaker_A': 'um, what it i at the end of the day, what it actually does is it picks those examples of the features that are closest to the separating boundary, and remembers those and and uses them to recreate the boundary for the test set.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_A': 'So, given these um these features, or or these these examples, um, critical examples, which they call support f support vectors,'",
        "'speaker_E': 'Oh.'",
        "'speaker_A': 'then um given a new example, if the new example falls um away from the boundary in one direction then it's classified as being a part of this particular class and otherwise it's the other class.'",
        "'speaker_E': 'So why save the examples?'",
        "'speaker_E': 'Why not just save what the'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_E': 'boundary itself is?'",
        "'speaker_A': 'Um.'",
        "'speaker_A': 'Hmm.'",
        "'speaker_A': 'Let's see.'",
        "'speaker_A': 'Uh.'",
        "'speaker_A': 'Yeah, that's a good question.'",
        "'speaker_A': 'I'",
        "'speaker_C': 'That's another way of doing it.'",
        "'speaker_A': 'yeah.'",
        "'speaker_E': 'Mmm.'",
        "'speaker_C': 'Right?'",
        "'speaker_C': 'So so it I mean I I guess it's'",
        "'speaker_E': 'Sort of an equivalent.'",
        "'speaker_C': 'You know, it it goes back to nearest neighbor'",
        "'speaker_C': 'sort of thing, right?'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'Um, i i if'",
        "'speaker_C': 'is it eh w'",
        "'speaker_C': 'When is nearest neighbor good?'",
        "'speaker_C': 'Well, nearest neighbor good is good if you have lots and lots of examples.'",
        "'speaker_C': 'Um but of course if you have lots and lots of examples, then it can take a while to to use nearest neighbor.'",
        "'speaker_C': 'There's lots of look ups.'",
        "'speaker_C': 'So a long time ago people talked about things where you would have'",
        "'speaker_C': 'uh a condensed nearest neighbor, where you would you would you would pick out uh some representative examples which would uh be sufficient to represent to to correctly classify everything that came in.'",
        "'speaker_E': 'Oh.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'I I think s I think support vector stuff sort of goes back to'",
        "'speaker_C': 'to that kind of thing.'",
        "'speaker_E': 'I see.'",
        "'speaker_C': 'Um.'",
        "'speaker_E': 'So rather than doing nearest neighbor where you compare to every single one,'",
        "'speaker_E': 'you just pick a few critical ones, and'",
        "'speaker_C': 'Yeah.'",
        "'speaker_E': 'Hmm.'",
        "'speaker_C': 'And th the'",
        "'speaker_C': 'You know, um'",
        "'speaker_C': 'neural net approach uh or Gaussian mixtures for that matter are sort of fairly brute force kinds of things, where you sort of'",
        "'speaker_C': 'you predefine that there is this big bunch of parameters and then you you place them as you best can to define the boundaries, and in fact, as you know,'",
        "'speaker_C': 'these things do take a lot of parameters and and uh if you have uh only a modest amount of data, you have trouble'",
        "'speaker_C': 'uh learning them.'",
        "'speaker_C': 'Um, so I I guess the idea to this is that it it is reputed to'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'uh be somewhat better in that regard.'",
        "'speaker_A': 'Right.'",
        "'speaker_A': 'I it can be a a reduced um'",
        "'speaker_A': 'parameterization of of the the model by just keeping certain selected examples.'",
        "'speaker_E': 'Hmm.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_A': 'So.'",
        "'speaker_C': 'But I don't know if people have done sort of careful comparisons of this on large tasks or anything.'",
        "'speaker_C': 'Maybe maybe they have.'",
        "'speaker_C': 'I don't know.'",
        "'speaker_A': 'Yeah, I don't know either.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_B': 'S do you get some kind of number between zero and one at the output?'",
        "'speaker_A': 'Actually you don't get a you don't get a nice number between zero and one.'",
        "'speaker_A': 'You get you get either a zero or a one.'",
        "'speaker_A': 'Um, uh there are there are pap Well, basically, it's it's um you you get a distance measure at the end of the day, and then that distance measure is is um is translated to a zero or one.'",
        "'speaker_A': 'Um.'",
        "'speaker_C': 'But that's looking at it for for classification for binary classification, right?'",
        "'speaker_A': 'That's for classification, right.'",
        "'speaker_E': 'And you get that for each class, you get a zero or a one.'",
        "'speaker_A': 'Right.'",
        "'speaker_C': 'But you have the distances to work with.'",
        "'speaker_A': 'You have the distances to work with, yeah.'",
        "'speaker_C': 'Cuz actually Mississippi State people did use support vector machines for uh uh speech recognition and they were using it to estimate probabilities.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_A': 'Yeah, they they had a had a way to translate the distances into into probabilities with the with the simple um'",
        "'speaker_A': 'uh sigmoidal function.'",
        "'speaker_C': 'Yeah, and d did they use sigmoid or a softmax type thing?'",
        "'speaker_C': 'And didn't they like exponentiate or something and then'",
        "'speaker_A': 'Um'",
        "'speaker_A': 'Yeah, there's some there's like one over one plus the exponential or something like that.'",
        "'speaker_C': 'divide by the sum of them, or?'",
        "'speaker_C': 'Oh it i Oh, so it is a sigmoidal.'",
        "'speaker_A': 'Yeah.'",
        "'speaker_C': 'OK.'",
        "'speaker_C': 'Alright.'",
        "'speaker_E': 'Did the'",
        "'speaker_E': 'did they get good results with that?'",
        "'speaker_C': 'I mean, they're OK, I I don't I don't think they were earth earth shattering, but I think that'",
        "'speaker_E': 'Hmm.'",
        "'speaker_C': 'uh this was a couple years ago, I remember them doing it at some meeting, and and um'",
        "'speaker_C': 'I don't think people were very critical because it was interesting just to to try this and you know, it was the first time they tried it, so'",
        "'speaker_E': 'Hmm.'",
        "'speaker_C': 'so the you know, the numbers were not incredibly good but there's you know, it was th reasonable.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'I I don't remember anymore.'",
        "'speaker_C': 'I don't even remember what the task was, it was Broadcast News, or'",
        "'speaker_C': 'something.'",
        "'speaker_C': 'I don't know.'",
        "'speaker_E': 'Hmm.'",
        "'speaker_A': 'Right.'",
        "'speaker_B': 'Uh s So Barry, if you just have zero and ones, how are you doing the speech recognition?'",
        "'speaker_A': 'Oh I'm not do I'm not planning on doing speech recognition with it.'",
        "'speaker_A': 'I'm just doing detection of phonological features.'",
        "'speaker_B': 'Oh.'",
        "'speaker_B': 'OK.'",
        "'speaker_A': 'So uh for example, this this uh feature set called the uh sound patterns of English'",
        "'speaker_A': 'um is just a bunch of um binary valued features.'",
        "'speaker_A': 'Let's say, is this voicing, or is this not voicing, is this'",
        "'speaker_A': 'sonorants, not sonorants, and'",
        "'speaker_B': 'OK.'",
        "'speaker_A': 'stuff like that.'",
        "'speaker_A': 'So.'",
        "'speaker_E': 'Did you find any more mistakes in their tables?'",
        "'speaker_A': 'Oh!'",
        "'speaker_A': 'Uh I haven't gone through the entire table, yet.'",
        "'speaker_A': 'Yeah, yesterday I brought Chuck the table and I was like, wait, this is'",
        "'speaker_A': 'Is the mapping fromN to to this phonological feature called um coronal\", is is should it be shouldn't it be a one?'",
        "'speaker_A': 'or should it should it be you know coronal instead of not coronal as it was labelled in the paper?'",
        "'speaker_A': 'So I ha haven't hunted down all the all the mistakes yet, but'",
        "'speaker_C': 'Uh huh.'",
        "'speaker_C': 'But a as I was saying, people do get probabilities from these things, and and uh we were just trying to remember how they do, but people have used it for speech recognition, and they have gotten probabilities.'",
        "'speaker_B': 'OK.'",
        "'speaker_C': 'So they have some conversion from these distances to'",
        "'speaker_B': 'OK.'",
        "'speaker_C': 'probabilities.'",
        "'speaker_A': 'Right, yeah.'",
        "'speaker_C': 'There's you have you have the paper, right?'",
        "'speaker_C': 'The Mississippi State paper?'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_C': 'Yeah, if you're interested y you could look, yeah.'",
        "'speaker_B': 'And OK.'",
        "'speaker_B': 'OK.'",
        "'speaker_A': 'Yeah, I can I can show you'",
        "'speaker_A': 'I yeah,'",
        "'speaker_E': 'So in your in in the thing that you're doing, uh'",
        "'speaker_A': 'our'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_E': 'you have a vector of ones and zeros for each phone?'",
        "'speaker_A': 'Uh, is this the class project, or?'",
        "'speaker_E': 'Yeah.'",
        "'speaker_A': 'OK.'",
        "'speaker_A': 'um'",
        "'speaker_E': 'Is that what you're'",
        "'speaker_A': 'Right, Right, right f so for every phone there is there is a um a vector of ones and zeros'",
        "'speaker_A': 'f uh corresponding to whether it exhibits a particular phonological feature or not.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_E': 'And so when you do your wh I'm what is the task for the class project?'",
        "'speaker_E': 'To'",
        "'speaker_A': 'Um'",
        "'speaker_E': 'come up with the phones?'",
        "'speaker_E': 'or to come up with these vectors to see how closely they match the phones, or?'",
        "'speaker_A': 'Oh.'",
        "'speaker_A': 'Right, um to come up with a mapping from um MFCC's or'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_A': 's some feature set,'",
        "'speaker_A': 'um to uh w to whether'",
        "'speaker_A': 'there's existence of a particular phonological feature.'",
        "'speaker_A': 'And um yeah, basically it's to learn a mapping from from the MFCC's to uh phonological features.'",
        "'speaker_A': 'Is it did that answer your question?'",
        "'speaker_E': 'I think so.'",
        "'speaker_A': 'OK.'",
        "'speaker_E': 'I guess'",
        "'speaker_A': 'C'",
        "'speaker_E': 'I mean, uh'",
        "'speaker_E': 'I'm not sure what you what you're what you get out of your system.'",
        "'speaker_E': 'Do you get out a'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_E': 'uh a vector of these ones and zeros and then try to find the closest matching phoneme to that vector, or?'",
        "'speaker_A': 'Oh.'",
        "'speaker_A': 'No, no.'",
        "'speaker_A': 'I'm not I'm not planning to do any any phoneme mapping yet.'",
        "'speaker_A': 'Just it's it's basically it's it's really simple, basically a detection of phonological features.'",
        "'speaker_E': 'Uh huh.'",
        "'speaker_E': 'I see.'",
        "'speaker_A': 'Yeah, and um'",
        "'speaker_A': 'cuz the uh So King and and Taylor um did this with uh recurrent neural nets, and this i their their idea was to first find a mapping from MFCC's to uh phonological features and then later on, once you have these phonological features,'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_A': 'then uh map that to phones.'",
        "'speaker_A': 'So I'm I'm sort of reproducing phase one of their stuff.'",
        "'speaker_E': 'Mmm.'",
        "'speaker_E': 'So they had one recurrent net for each particular feature?'",
        "'speaker_A': 'Right.'",
        "'speaker_A': 'Right.'",
        "'speaker_E': 'I see.'",
        "'speaker_A': 'Right.'",
        "'speaker_A': 'Right.'",
        "'speaker_E': 'I wo did they compare that I mean, what if you just did phone recognition and did the reverse lookup.'",
        "'speaker_A': 'Uh.'",
        "'speaker_E': 'So you recognize a phone and which ever phone was recognized, you spit out it's'",
        "'speaker_A': 'Mm hmm.'",
        "'speaker_E': 'vector of ones and zeros.'",
        "'speaker_C': 'I expect you could do that.'",
        "'speaker_E': 'I mean uh'",
        "'speaker_A': 'Uh.'",
        "'speaker_C': 'That's probably not what he's going to do on his class project.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_E': 'No.'"
    ],
    "update from skeaker_E, SRI system normalization": [
        "'speaker_C': 'So um have you had a chance to do this um thing we talked about yet with the uh'",
        "'speaker_A': 'Yeah.'",
        "'speaker_C': 'um'",
        "'speaker_E': 'Insertion penalty?'",
        "'speaker_C': 'Uh.'",
        "'speaker_C': 'No actually I was going a different That's a good question, too, but I was gonna ask about the'",
        "'speaker_C': 'the um'",
        "'speaker_C': 'changes to the data in comparing PLP and mel cepstrum'",
        "'speaker_C': 'for the SRI'",
        "'speaker_C': 'system.'",
        "'speaker_E': 'Uh.'",
        "'speaker_E': 'Well what I've been'",
        "'speaker_E': '\" Changes to the data\", I'm not sure I'",
        "'speaker_C': 'Right.'",
        "'speaker_C': 'So we talked on the phone about this, that that there was still a difference of a of a few percent and'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'Right.'",
        "'speaker_C': 'you told me that there was a difference in how the normalization was done.'",
        "'speaker_C': 'And I was asking if you were going to do'",
        "'speaker_C': 'redo it uh for PLP with the normalization done as it had been done for the mel cepstrum.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_E': 'Uh'",
        "'speaker_E': 'right, no I haven't had a chance to do that.'",
        "'speaker_E': 'What I've been doing is'",
        "'speaker_C': 'OK.'",
        "'speaker_E': 'uh'",
        "'speaker_E': 'trying to figure out it just seems to me like there's a um well it seems like there's a bug, because'",
        "'speaker_E': 'the difference in performance is it's not gigantic'",
        "'speaker_E': 'but it's big enough that it it seems wrong.'",
        "'speaker_C': 'Yeah, I agree, but I thought that the normalization difference was one of the'",
        "'speaker_E': 'and'",
        "'speaker_E': 'Yeah, but I don't I'm not Yeah, I guess I don't think that the normalization difference is gonna account for everything.'",
        "'speaker_C': 'possibilities, right?'",
        "'speaker_E': 'So what I was working on is um'",
        "'speaker_C': 'OK.'",
        "'speaker_E': 'just going through and checking the headers of the wavefiles, to see if'",
        "'speaker_E': 'maybe there was a um'",
        "'speaker_E': 'a certain type of compression or something that was done that my script wasn't catching.'",
        "'speaker_E': 'So that for some subset of'",
        "'speaker_E': 'the training data, uh the the the features I was computing were junk.'",
        "'speaker_C': 'OK.'",
        "'speaker_E': 'Which would you know'",
        "'speaker_E': 'cause it to perform OK, but uh, you know, the the models would be all messed up.'",
        "'speaker_E': 'So I was going through and just double checking that kind of think first, to see if there was just some kind of obvious bug in the way that I was computing the features.'",
        "'speaker_C': 'Mm hmm.'",
        "'speaker_C': 'I see.'",
        "'speaker_C': 'OK.'",
        "'speaker_E': 'Looking at all the sampling rates to make sure all the sampling rates were what'",
        "'speaker_C': 'Yeah.'",
        "'speaker_E': 'eightK, what I was assuming they were, um'",
        "'speaker_C': 'Yeah, that makes sense, to check all that.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'So I was doing that first, before I'",
        "'speaker_E': 'did these other things, just to make sure there wasn't something'",
        "'speaker_C': 'Although really, uh uh, a couple three percent uh difference in word error rate uh could easily come from some difference in normalization, I would think.'",
        "'speaker_C': 'But'",
        "'speaker_E': 'Yeah, and I think, I'm trying to remember but I think I recall that Andreas was saying that he was gonna run'",
        "'speaker_E': 'sort of the reverse experiment.'",
        "'speaker_E': 'Uh'",
        "'speaker_E': 'which is to'",
        "'speaker_E': 'try to'",
        "'speaker_E': 'emulate the'",
        "'speaker_E': 'normalization that we did but with the mel cepstral features.'",
        "'speaker_E': 'Sort of, you know,'",
        "'speaker_E': 'back up from the system that he had.'",
        "'speaker_E': 'I thought he said he was gonna I have to look back through my my email from him.'",
        "'speaker_C': 'Yeah, he's probably off at at uh his meeting now, yeah.'",
        "'speaker_E': 'Yeah, he's gone now.'",
        "'speaker_E': 'Um.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_C': 'But yeah the I sh think they should be'",
        "'speaker_E': 'But'",
        "'speaker_C': 'roughly equivalent, um'",
        "'speaker_C': 'I mean again'",
        "'speaker_C': 'the Cambridge folk found the PLP actually to be a little better.'",
        "'speaker_E': 'Right.'",
        "'speaker_C': 'Uh So it's'",
        "'speaker_C': 'um I mean the other thing I wonder about was whether there was something just in the the bootstrapping of their system which was based on but maybe not, since they'",
        "'speaker_E': 'Yeah see one thing that's a little bit um'",
        "'speaker_E': 'I was looking I've been studying and going through the logs for the system that um Andreas created.'",
        "'speaker_E': 'And um'",
        "'speaker_E': 'his uh the way that the'",
        "'speaker_E': 'SR system looks like it works is that it reads the wavefiles directly, uh'",
        "'speaker_C': 'Right.'",
        "'speaker_E': 'and does all of the cepstral computation stuff on the fly.'",
        "'speaker_C': 'Right.'",
        "'speaker_E': 'And, so there's no place where these'",
        "'speaker_E': 'where the cepstral files are stored, anywhere that I can go look at and compare to the PLP ones, so'",
        "'speaker_E': 'whereas with our features, he's actually storing'",
        "'speaker_E': 'the cepstrum on disk, and he reads those in.'",
        "'speaker_C': 'Right.'",
        "'speaker_E': 'But it looked like he had to give it'",
        "'speaker_E': 'uh even though the cepstrum is already computed, he has to give it uh'",
        "'speaker_E': 'a front end parameter file.'",
        "'speaker_E': 'Which talks about the kind of'",
        "'speaker_E': 'uh'",
        "'speaker_E': 'com computation that his mel cepstrum thing does, so'",
        "'speaker_C': 'Uh huh.'",
        "'speaker_E': 'i I I don't know if that it probably doesn't mess it up, it probably just ignores it if it determines that it's already in the right format or something but the the the two processes that happen are a little different.'",
        "'speaker_E': 'So.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_C': 'So anyway, there's stuff there to sort out.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_C': 'So, OK.'",
        "'speaker_C': 'Let's go back to what you thought I was asking you.'",
        "'speaker_E': 'Yeah no and I didn't have a chance to do that.'",
        "'speaker_C': 'Ha!'",
        "'speaker_C': 'Oh!'",
        "'speaker_C': 'You had the sa same answer anyway.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'I've been um, I've been working with um'",
        "'speaker_E': 'Jeremy on his project and then I've been trying to track down this bug'",
        "'speaker_E': 'in uh'",
        "'speaker_C': 'Uh huh.'",
        "'speaker_E': 'the ICSI front end features.'",
        "'speaker_E': 'So one thing that I did notice, yesterday I was studying the um'",
        "'speaker_E': 'the uh RASTA code'",
        "'speaker_C': 'Uh huh.'",
        "'speaker_E': 'and it looks like we don't have any way to um'",
        "'speaker_E': 'control the frequency range that we use in our analysis.'",
        "'speaker_E': 'We basically it looks to me like we do the FFT, um and then we just take all the bins and we use everything.'",
        "'speaker_E': 'We don't have any set of parameters where we can say you know, only process from you know a hundred and ten hertz to thirty fifty\".'",
        "'speaker_C': 'Um'",
        "'speaker_E': 'At least I couldn't see any kind of control for that.'",
        "'speaker_C': 'Yeah, I don't think it's in there, I think it's in the uh uh'",
        "'speaker_C': 'uh the filters.'",
        "'speaker_C': 'So, theFF is on everything, but the filters'",
        "'speaker_C': 'um, for instance, ignore the the lowest'",
        "'speaker_C': 'bins'",
        "'speaker_C': 'and the highest bins.'",
        "'speaker_C': 'And what it does is it it copies'",
        "'speaker_E': 'The the filters?'",
        "'speaker_E': 'Which filters?'",
        "'speaker_C': 'um'",
        "'speaker_C': 'The filter bank which is created by integrating overFF bins.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'um'",
        "'speaker_E': 'When you get the mel'",
        "'speaker_E': 'When you go to the mel scale.'",
        "'speaker_C': 'Right.'",
        "'speaker_C': 'Yeah, it's bark scale, and it's it it um it actually copies'",
        "'speaker_C': 'the uh um the second'",
        "'speaker_C': 'filters over to the first.'",
        "'speaker_C': 'So the first filters are always and you can s you can specify a different number of'",
        "'speaker_C': 'uh features different number of filters, I think, as I recall.'",
        "'speaker_C': 'So you can specify a different number of filters, and whatever'",
        "'speaker_C': 'um'",
        "'speaker_C': 'uh you specify, the last ones are gonna be ignored.'",
        "'speaker_C': 'So that that's a way that you sort of'",
        "'speaker_C': 'change what the what the bandwidth is.'",
        "'speaker_C': 'Y you can't do it without I think changing the number of filters, but'",
        "'speaker_E': 'I saw something about uh'",
        "'speaker_E': 'that looked like it was doing something like that, but I didn't quite understand it.'",
        "'speaker_E': 'So maybe'",
        "'speaker_C': 'Yeah, so the idea is that the very lowest frequencies and and typically the veriest highest frequencies are kind of junk.'",
        "'speaker_E': 'Uh huh.'",
        "'speaker_C': 'And so um you just for continuity you just approximate them by'",
        "'speaker_C': 'by the second to highest and second to lowest.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'It's just a simple thing we put in.'",
        "'speaker_C': 'And and so if you h'",
        "'speaker_E': 'But so the but that's a fixed uh thing?'",
        "'speaker_E': 'There's nothing that lets you'",
        "'speaker_C': 'Yeah, I think that's a fixed thing.'",
        "'speaker_C': 'But see see my point?'",
        "'speaker_C': 'If you had'",
        "'speaker_C': 'If you had ten filters,'",
        "'speaker_C': 'then you would be throwing away a lot at the two ends.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'And if you had if you had fifty filters, you'd be throwing away hardly anything.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'Um, I don't remember there being an independent way of saying we're just gonna'",
        "'speaker_C': 'make them from here to here\".'",
        "'speaker_E': 'Use this analysis bandwidth or something.'",
        "'speaker_C': 'But I I I don't know, it's actually been awhile since I've looked at it.'",
        "'speaker_E': 'Yeah, I went through the Feacalc code and then looked at you know just calling the RASTA libs and thing like that.'",
        "'speaker_E': 'And I didn't I couldn't see any wh place where that kind of thing was done.'",
        "'speaker_E': 'But um I didn't quite understand everything that I saw, so'",
        "'speaker_C': 'Yeah, see I don't know Feacalc at all.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'But it calls RASTA with some options, and um'",
        "'speaker_E': 'Right.'",
        "'speaker_C': 'But I I think in I don't know.'",
        "'speaker_C': 'I guess for some particular database you might find that you could tune that and tweak that to get that a little better, but I think that'",
        "'speaker_C': 'in general it's not'",
        "'speaker_C': 'that critical.'",
        "'speaker_C': 'I mean there's'",
        "'speaker_E': 'Yeah.'",
        "'speaker_C': 'You can You can throw away stuff below a hundred hertz or so and it's just not going to affect phonetic classification at all.'",
        "'speaker_E': 'Another thing I was thinking about was um is there a'",
        "'speaker_E': 'I was wondering if there's maybe um certain settings of the parameters when you compute PLP which would basically cause it to output mel cepstrum.'",
        "'speaker_E': 'So that, in effect, what I could do is use our code but produce mel cepstrum'",
        "'speaker_E': 'and compare that directly to'",
        "'speaker_C': 'Well, it's not precisely.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_E': 'Hmm.'",
        "'speaker_C': 'I mean, um,'",
        "'speaker_C': 'um'",
        "'speaker_C': 'what you can do'",
        "'speaker_C': 'is um'",
        "'speaker_C': 'you can definitely change the the filter bank from being uh a uh trapezoidal integration to a a a triangular one,'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'which is what the typical mel mel cepstral uh filter bank does.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'And some people have claimed that they got some better performance doing that, so you certainly could do that'",
        "'speaker_C': 'easily.'",
        "'speaker_C': 'But the fundamental difference, I mean, there's other small differences'",
        "'speaker_E': 'There's a cubic root that happens, right?'",
        "'speaker_C': 'Yeah, but, you know, as opposed to the log in the other case.'",
        "'speaker_C': 'I mean the fundamental d d difference that we've seen any kind of difference from before, which is actually an advantage for thePL i uh, I think, is that the the smoothing at the end is auto regressive instead of being cepstral uh, from cepstral truncation.'",
        "'speaker_C': 'So um it's a little more noise robust.'",
        "'speaker_E': 'Hmm.'",
        "'speaker_C': 'Um, and that's that's why when people started getting databases that had a little more noise in it, like like uh um Broadcast News and so on, that's why c Cambridge switched to PLP I think.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'So um'",
        "'speaker_C': 'That's a difference that I don't'",
        "'speaker_C': 'think we put any way to get around, since it was an advantage.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'um'",
        "'speaker_C': 'uh but we did eh we did hear this comment from people at some point, that'",
        "'speaker_C': 'um it uh they got some better results with the triangular filters rather than the trapezoidal.'",
        "'speaker_C': 'So that is an option in RASTA.'",
        "'speaker_E': 'Hmm.'",
        "'speaker_C': 'Uh and you can certainly play with that.'",
        "'speaker_C': 'But I think you're probably doing the right thing to look for bugs first.'",
        "'speaker_E': 'Yeah just it just seems like this kind of behavior could be caused by'",
        "'speaker_C': 'I don't know.'",
        "'speaker_E': 'you know'",
        "'speaker_E': 's some of the training data being messed up.'",
        "'speaker_C': 'Could be.'",
        "'speaker_E': 'You know, you're sort of getting most of the way there, but there's a'",
        "'speaker_E': 'So I started going through and looking One of the things that I did notice was that the um'",
        "'speaker_E': 'log likelihoods coming out of the log recognizer from the PLP data'",
        "'speaker_E': 'were much lower, much smaller, than for the mel cepstral stuff,'",
        "'speaker_E': 'and that the average amount of pruning that was happening was therefore a little bit higher'",
        "'speaker_E': 'for the PLP features.'",
        "'speaker_C': 'Oh huh!'",
        "'speaker_E': 'So, since he used the same exact pruning thresholds for both,'",
        "'speaker_E': 'I was wondering if it could be that we're getting more pruning.'",
        "'speaker_C': 'Oh!'",
        "'speaker_C': 'He he He used the identical pruning thresholds even though the s the range of p of the likeli Oh well that's'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'Right.'",
        "'speaker_E': 'Right.'",
        "'speaker_C': 'That's a pretty good point right there.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_E': 'Yeah, so'",
        "'speaker_C': 'I would think that you might wanna do something like uh'",
        "'speaker_C': 'you know, look at a few points to see where you are starting to get significant search errors.'",
        "'speaker_E': 'That's'",
        "'speaker_E': 'Right.'",
        "'speaker_E': 'Well, what I was gonna do is I was gonna take um a couple of the utterances that he had run through,'",
        "'speaker_E': 'then'",
        "'speaker_E': 'run them through again but modify the pruning threshold and see if it'",
        "'speaker_E': 'you know, affects the score.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_E': 'So.'",
        "'speaker_C': 'But I mean you could uh if if if that looks promising you could, you know, r uh run'",
        "'speaker_C': 'the overall test set with a with a few different uh pruning thresholds for both,'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_E': 'Right.'",
        "'speaker_C': 'and presumably he's running at some pruning threshold that's that's uh, you know gets very few search errors but is is relatively fast and'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_E': 'Right.'",
        "'speaker_E': 'I mean, yeah, generally in these things you'",
        "'speaker_E': 'you turn back pruning really far, so I I didn't think it would be that big a deal because I was figuring well you have it turned back so far that'",
        "'speaker_E': 'you know it'",
        "'speaker_C': 'But you may be in the wrong range for thePL features for some reason.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'And the uh the the run time of the recognizer on the PLP features is longer which sort of implies that the networks are bushier, you know, there's more things it's considering which goes along with the fact that the matches aren't as good.'",
        "'speaker_E': 'So uh, you know, it could be that we're just pruning'",
        "'speaker_E': 'too much.'",
        "'speaker_E': 'So.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_C': 'Yeah, maybe just be different kind of distributions and and yeah so that's another possible thing.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'They they should really shouldn't There's no particular reason why they would be'",
        "'speaker_C': 'exactly behave exactly the same.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_E': 'Right.'",
        "'speaker_E': 'Right.'",
        "'speaker_C': 'So.'",
        "'speaker_E': 'So.'",
        "'speaker_E': 'There's lots of little differences.'",
        "'speaker_E': 'So.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_E': 'Uh.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_E': 'Trying to track it down.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_C': 'I guess this was a little bit off topic, I guess, because I was I was thinking in terms of th this as being a a a a core'",
        "'speaker_E': 'Yeah'",
        "'speaker_C': 'item that once we once we had it going we would use for a number of the front end things also.'",
        "'speaker_E': 'Mm hmm.'",
        "'speaker_C': 'So.'"
    ],
    "closing": [
        "'speaker_C': 'Alright, um, I actually have to run.'",
        "'speaker_C': 'So I don't think I can do the digits, but um,'",
        "'speaker_C': 'I guess I'll leave my microphone on?'",
        "'speaker_E': 'Uh, yeah.'",
        "'speaker_C': 'Yeah.'",
        "'speaker_C': 'Thank you.'",
        "'speaker_E': 'Yep.'",
        "'speaker_E': 'Yeah.'",
        "'speaker_E': 'That'll work.'",
        "'speaker_C': 'I.'",
        "'speaker_C': 'That's.'",
        "'speaker_C': 'I.'",
        "'speaker_C': 'OK, I.'",
        "'speaker_C': 'left it on.'",
        "'speaker_C': 'OK.'"
    ]
}